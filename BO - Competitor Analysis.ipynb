{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "# Display all columns of a pandas dataframe\n",
    "# pd.set_option('display.max_columns', 0)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_all_business_df = pd.read_csv('data/toronto_all_business.csv')\n",
    "toronto_restaurant_business_df = pd.read_csv('data/toronto_restaurant_business.csv')\n",
    "toronto_japanese_business_df = pd.read_csv('data/toronto_japanese_business.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Competitor Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The client of our project has been selected based both on the __number of stars (rating)__ and also **on the total number of reviews**_. \n",
    "\n",
    "We assume that the total number of reviews gives a strong indication of the popularity of the business, and we have selected a restaurant business with high number of ratings, since we are interested in having enough reviews to perform a comprehensive text analysis review upon the client. \n",
    "\n",
    "Moreover, we want to select a business with an average rating of 4.0/5.0, since we are interested in advising a client on potential ways to inprove its outreach and services into the targeted market. A business with large amount of reviews and ratings of 4.5 or 5.0 stars are highly difficult to achieve improvement with, since their ranking is already quite high, where improvments are normally found in small details, and perhaps small issues or negative reviews, even with such high overall ratings, are due to the law of large numbers and the vast preferences of customers.\n",
    "\n",
    "__Our client is: Kinka Izakaya Original, a Japanese izakaya restaurant/bar in Toronto.__\n",
    "\n",
    "The client is a Japanese izakaya restaurant which focuses on an evening schedule and cuisine in order to attract customers. The term \"izakaya\" comes from a Japanese concept of a traditional tavern with lively, cheerful atmosphere. An izakaya is well-suited both for after-work corporate small drinking events and also for family dinners. The atmosphere in most izakayas is family-friendly, with a strong emphasis on having a good time with the people around you, whether it be family members or good friends. That is why most izakayas also offer small food plates with traditional food, and servings are considered as important as the drinks themselves. This is much in contrast to bars and night restaurants, where the atmosphere is heavily focused on drinking, and is not a family-friendly environment. Therefore, our client distinguishes itself from some other evening restaurant through the thematic and the concept upon which the restaurant is centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_row = (toronto_japanese_business_df[toronto_japanese_business_df.name == \"KINKA IZAKAYA ORIGINAL\"]).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurants, Tapas/Small Plates, Japanese, Bars, Pubs, Tapas Bars, Nightlife\n"
     ]
    }
   ],
   "source": [
    "# Let us print the categories of restaurant that the client is involved in:\n",
    "print(client_row.categories)\n",
    "\n",
    "category_list_client = client_row.categories.split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Direct competitors with very similar business categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to know your own competition, and some of the most relevant examples of competitors are restaurants with similar or identical cuisine specific as our client has. These restaurants serve as direct competitors, no matter their size or location, since they attack the same customer market as our client does. \n",
    "\n",
    "We will find these competitors by looking at the number of common categories of business. Our client has 7 of them, and we will consider that if another business tackles at least 5 out of 7 of these same categories, they become our client's competitors. \n",
    "\n",
    "To ease our understanding of how close a business is competing with our client in terms of actual cuisine, we have considered 3 levels of direct competition:\n",
    "\n",
    "* ___1st level competitors: businesses which tackle completely the same kind of customers (7/7 client categories match)___\n",
    "* ___2nd level competitors: businesses which are very similar to our client's (6/7 client categories matched)___\n",
    "* ___3rd level competitors: businesses which attack a large part of our client's customer market (5/7 client categories matched)___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor_common_categories_df = pd.DataFrame(columns = ['index', 'nr_common_categories', 'list_common_categories'])\n",
    "\n",
    "# This for-loop iterates through all the list-format categories of every business in Toronot\n",
    "for index, row in toronto_all_business_df.apply(lambda row: row.categories.split(', '), axis = 1).iteritems():\n",
    "\n",
    "    # If the intersection between the client categories and the other business' categories is constituded of at least 1 category, then:\n",
    "    if (len(set(row) & set(category_list_client)) > 0):\n",
    "        \n",
    "        # then append its dataframe index, the number of categories in common with the client and also their actual values to a new dataframe\n",
    "        competitor_common_categories_df = competitor_common_categories_df.append({\n",
    "                                               'index': index, \n",
    "                                               'nr_common_categories': len(set(row) & set(category_list_client)), \n",
    "                                               'list_common_categories': list(set(row) & set(category_list_client))\n",
    "                                               }, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the values in the dataframe descendingly, by the number of common categories\n",
    "\n",
    "competitor_common_categories_df = competitor_common_categories_df.sort_values(by = 'nr_common_categories', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nr_of_businesses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nr_common_categories</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      nr_of_businesses\n",
       "nr_common_categories                  \n",
       "7                                    4\n",
       "6                                    2\n",
       "5                                   32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how many competitors have at least 5 out 7 common categories with our client business:\n",
    "\n",
    "competitor_common_categories_df[competitor_common_categories_df.nr_common_categories >= 5] \\\n",
    "    .groupby('nr_common_categories') \\\n",
    "    .count() \\\n",
    "    .sort_index(ascending = False)[['index']] \\\n",
    "    .rename(columns = {'index': 'nr_of_businesses'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **38 main direct competitors of *Kinka Izakaya Original***, out of which __4__ are offering _exactly_ the same field of cuisine as our client (1st level: ___7/7 categories matched___), __2__ of them are almost identical in terms of business orientation (2nd level: ___6/7 categories matched___), and __32__ other restaurants are directly competing with a client by having a highly-overlapping set of business categories (3rd level: ___5/7 categories matched___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>nr_common_categories</th>\n",
       "      <th>list_common_categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8489</td>\n",
       "      <td>18485</td>\n",
       "      <td>7</td>\n",
       "      <td>[Nightlife, Bars, Tapas Bars, Tapas/Small Plat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8198</td>\n",
       "      <td>17836</td>\n",
       "      <td>7</td>\n",
       "      <td>[Nightlife, Bars, Tapas Bars, Tapas/Small Plat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5371</td>\n",
       "      <td>11730</td>\n",
       "      <td>7</td>\n",
       "      <td>[Nightlife, Bars, Tapas Bars, Tapas/Small Plat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>1081</td>\n",
       "      <td>7</td>\n",
       "      <td>[Nightlife, Bars, Tapas Bars, Tapas/Small Plat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2353</td>\n",
       "      <td>6</td>\n",
       "      <td>[Nightlife, Bars, Tapas Bars, Tapas/Small Plat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3518</td>\n",
       "      <td>7610</td>\n",
       "      <td>1</td>\n",
       "      <td>[Restaurants]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3517</td>\n",
       "      <td>7608</td>\n",
       "      <td>1</td>\n",
       "      <td>[Restaurants]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3516</td>\n",
       "      <td>7606</td>\n",
       "      <td>1</td>\n",
       "      <td>[Restaurants]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3515</td>\n",
       "      <td>7603</td>\n",
       "      <td>1</td>\n",
       "      <td>[Restaurants]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9325</td>\n",
       "      <td>20328</td>\n",
       "      <td>1</td>\n",
       "      <td>[Restaurants]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9326 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index nr_common_categories  \\\n",
       "8489  18485                    7   \n",
       "8198  17836                    7   \n",
       "5371  11730                    7   \n",
       "491    1081                    7   \n",
       "1060   2353                    6   \n",
       "...     ...                  ...   \n",
       "3518   7610                    1   \n",
       "3517   7608                    1   \n",
       "3516   7606                    1   \n",
       "3515   7603                    1   \n",
       "9325  20328                    1   \n",
       "\n",
       "                                 list_common_categories  \n",
       "8489  [Nightlife, Bars, Tapas Bars, Tapas/Small Plat...  \n",
       "8198  [Nightlife, Bars, Tapas Bars, Tapas/Small Plat...  \n",
       "5371  [Nightlife, Bars, Tapas Bars, Tapas/Small Plat...  \n",
       "491   [Nightlife, Bars, Tapas Bars, Tapas/Small Plat...  \n",
       "1060  [Nightlife, Bars, Tapas Bars, Tapas/Small Plat...  \n",
       "...                                                 ...  \n",
       "3518                                      [Restaurants]  \n",
       "3517                                      [Restaurants]  \n",
       "3516                                      [Restaurants]  \n",
       "3515                                      [Restaurants]  \n",
       "9325                                      [Restaurants]  \n",
       "\n",
       "[9326 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitor_common_categories_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will separate the businesses according to the 3 different levels of direct competition mentioned above, depending on how close their target market is compared to the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor_direct_1st_level = pd.DataFrame()\n",
    "competitor_direct_2nd_level = pd.DataFrame()\n",
    "competitor_direct_3rd_level = pd.DataFrame()\n",
    "\n",
    "for index in competitor_common_categories_df[competitor_common_categories_df.nr_common_categories == 7]['index']:\n",
    "    competitor_direct_1st_level = competitor_direct_1st_level.append(toronto_all_business_df[toronto_all_business_df.index == index])\n",
    "    \n",
    "for index in competitor_common_categories_df[competitor_common_categories_df.nr_common_categories == 6]['index']:\n",
    "    competitor_direct_2nd_level = competitor_direct_2nd_level.append(toronto_all_business_df[toronto_all_business_df.index == index])\n",
    "    \n",
    "for index in competitor_common_categories_df[competitor_common_categories_df.nr_common_categories == 5]['index']:\n",
    "    competitor_direct_3rd_level = competitor_direct_3rd_level.append(toronto_all_business_df[toronto_all_business_df.index == index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18485</td>\n",
       "      <td>CN5nuUQod0f8g3oh99qq0w</td>\n",
       "      <td>KINKA IZAKAYA ANNEX</td>\n",
       "      <td>559 Bloor St W</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M5S 1Y6</td>\n",
       "      <td>43.665157</td>\n",
       "      <td>-79.410658</td>\n",
       "      <td>4.0</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"HasTV\": \"False\", \"BikeParking\": \"True\", \"Goo...</td>\n",
       "      <td>Tapas/Small Plates, Tapas Bars, Japanese, Loca...</td>\n",
       "      <td>{\"Monday\": \"17:00-0:00\", \"Tuesday\": \"17:00-0:0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17836</td>\n",
       "      <td>RtUvSWO_UZ8V3Wpj0n077w</td>\n",
       "      <td>KINKA IZAKAYA ORIGINAL</td>\n",
       "      <td>398 Church St</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M5B 2A2</td>\n",
       "      <td>43.660430</td>\n",
       "      <td>-79.378927</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1592</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"RestaurantsAttire\": \"casual\", \"BusinessParki...</td>\n",
       "      <td>Restaurants, Tapas/Small Plates, Japanese, Bar...</td>\n",
       "      <td>{\"Monday\": \"17:00-0:00\", \"Tuesday\": \"17:00-0:0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11730</td>\n",
       "      <td>igo--IP_NBWIO_yGtBJ0Eg</td>\n",
       "      <td>Ogadang</td>\n",
       "      <td>41 Spring Garden Avenue</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M2N 5N6</td>\n",
       "      <td>43.764552</td>\n",
       "      <td>-79.410009</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"Ambience\": {\"touristy\": \"False\", \"hipster\": ...</td>\n",
       "      <td>Restaurants, Japanese, Pubs, Tapas/Small Plate...</td>\n",
       "      <td>{\"Tuesday\": \"17:30-2:30\", \"Wednesday\": \"17:30-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1081</td>\n",
       "      <td>CfxVkwEJk1NAqgqMSesLzA</td>\n",
       "      <td>KINKA IZAKAYA NORTH  YORK</td>\n",
       "      <td>4775 Yonge Street, Unit 114</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M2N 5M5</td>\n",
       "      <td>43.760303</td>\n",
       "      <td>-79.409798</td>\n",
       "      <td>3.5</td>\n",
       "      <td>375</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"WiFi\": \"no\", \"CoatCheck\": \"False\", \"OutdoorS...</td>\n",
       "      <td>Japanese, Restaurants, Tapas/Small Plates, Pub...</td>\n",
       "      <td>{\"Monday\": \"17:00-0:00\", \"Tuesday\": \"17:00-0:0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  business_id                       name  \\\n",
       "18485  CN5nuUQod0f8g3oh99qq0w        KINKA IZAKAYA ANNEX   \n",
       "17836  RtUvSWO_UZ8V3Wpj0n077w     KINKA IZAKAYA ORIGINAL   \n",
       "11730  igo--IP_NBWIO_yGtBJ0Eg                    Ogadang   \n",
       "1081   CfxVkwEJk1NAqgqMSesLzA  KINKA IZAKAYA NORTH  YORK   \n",
       "\n",
       "                           address     city state postal_code   latitude  \\\n",
       "18485               559 Bloor St W  Toronto    ON     M5S 1Y6  43.665157   \n",
       "17836                398 Church St  Toronto    ON     M5B 2A2  43.660430   \n",
       "11730      41 Spring Garden Avenue  Toronto    ON     M2N 5N6  43.764552   \n",
       "1081   4775 Yonge Street, Unit 114  Toronto    ON     M2N 5M5  43.760303   \n",
       "\n",
       "       longitude  stars  review_count  is_open  \\\n",
       "18485 -79.410658    4.0           461        1   \n",
       "17836 -79.378927    4.0          1592        1   \n",
       "11730 -79.410009    4.0            18        1   \n",
       "1081  -79.409798    3.5           375        1   \n",
       "\n",
       "                                              attributes  \\\n",
       "18485  {\"HasTV\": \"False\", \"BikeParking\": \"True\", \"Goo...   \n",
       "17836  {\"RestaurantsAttire\": \"casual\", \"BusinessParki...   \n",
       "11730  {\"Ambience\": {\"touristy\": \"False\", \"hipster\": ...   \n",
       "1081   {\"WiFi\": \"no\", \"CoatCheck\": \"False\", \"OutdoorS...   \n",
       "\n",
       "                                              categories  \\\n",
       "18485  Tapas/Small Plates, Tapas Bars, Japanese, Loca...   \n",
       "17836  Restaurants, Tapas/Small Plates, Japanese, Bar...   \n",
       "11730  Restaurants, Japanese, Pubs, Tapas/Small Plate...   \n",
       "1081   Japanese, Restaurants, Tapas/Small Plates, Pub...   \n",
       "\n",
       "                                                   hours  \n",
       "18485  {\"Monday\": \"17:00-0:00\", \"Tuesday\": \"17:00-0:0...  \n",
       "17836  {\"Monday\": \"17:00-0:00\", \"Tuesday\": \"17:00-0:0...  \n",
       "11730  {\"Tuesday\": \"17:30-2:30\", \"Wednesday\": \"17:30-...  \n",
       "1081   {\"Monday\": \"17:00-0:00\", \"Tuesday\": \"17:00-0:0...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitor_direct_1st_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first level of direct competition, we can see that the Kinka Izakaya Original restaurant is competing with its other two smaller branches __Kinka Izakaya Annex__ & __Kinka Izakaya North York__, all of which are in Toronto. Moreover, another sub-branch of the business is a second level competitor. Since they are not competing restaurant, but rather different branches, they should be analyzed separately and will thus be put into a separate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinka_all_branches_df = toronto_all_business_df[toronto_all_business_df.name.str.contains('KINKA')]\n",
    "\n",
    "competitor_direct_1st_level = competitor_direct_1st_level[competitor_direct_1st_level['name'].isin(kinka_all_branches_df['name'].values) == False]\n",
    "competitor_direct_2nd_level = competitor_direct_2nd_level[competitor_direct_2nd_level['name'].isin(kinka_all_branches_df['name'].values) == False]\n",
    "competitor_direct_3rd_level = competitor_direct_3rd_level[competitor_direct_3rd_level['name'].isin(kinka_all_branches_df['name'].values) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see that only 1 other restaurant in Toronto shares exactly the same business categories as our client's izakaya business model, and also happens to have the same average rating of 4 stars, albeit from a much smaller set of reviews. These considerations make ___Ogadang___ one of our clients most direct competitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11730</td>\n",
       "      <td>igo--IP_NBWIO_yGtBJ0Eg</td>\n",
       "      <td>Ogadang</td>\n",
       "      <td>41 Spring Garden Avenue</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M2N 5N6</td>\n",
       "      <td>43.764552</td>\n",
       "      <td>-79.410009</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"Ambience\": {\"touristy\": \"False\", \"hipster\": ...</td>\n",
       "      <td>Restaurants, Japanese, Pubs, Tapas/Small Plate...</td>\n",
       "      <td>{\"Tuesday\": \"17:30-2:30\", \"Wednesday\": \"17:30-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  business_id     name                  address     city  \\\n",
       "11730  igo--IP_NBWIO_yGtBJ0Eg  Ogadang  41 Spring Garden Avenue  Toronto   \n",
       "\n",
       "      state postal_code   latitude  longitude  stars  review_count  is_open  \\\n",
       "11730    ON     M2N 5N6  43.764552 -79.410009    4.0            18        1   \n",
       "\n",
       "                                              attributes  \\\n",
       "11730  {\"Ambience\": {\"touristy\": \"False\", \"hipster\": ...   \n",
       "\n",
       "                                              categories  \\\n",
       "11730  Restaurants, Japanese, Pubs, Tapas/Small Plate...   \n",
       "\n",
       "                                                   hours  \n",
       "11730  {\"Tuesday\": \"17:30-2:30\", \"Wednesday\": \"17:30-...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitor_direct_1st_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, at the 2nd level of direct competition, we only have one other restaurant which has 6/7 common categories with our client, however this restaurant also seems to be not only much less popular, but it is out-of-business as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2353</td>\n",
       "      <td>996axDvF9P8zxB9MJA52Xg</td>\n",
       "      <td>Yuzu Izakaya</td>\n",
       "      <td>5582 Yonge Street</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M2N</td>\n",
       "      <td>43.779041</td>\n",
       "      <td>-79.415528</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"RestaurantsPriceRange2\": \"2\", \"GoodForKids\":...</td>\n",
       "      <td>Tapas/Small Plates, Tapas Bars, Nightlife, Res...</td>\n",
       "      <td>{\"Monday\": \"17:30-2:30\", \"Tuesday\": \"17:30-2:3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id          name            address     city state  \\\n",
       "2353  996axDvF9P8zxB9MJA52Xg  Yuzu Izakaya  5582 Yonge Street  Toronto    ON   \n",
       "\n",
       "     postal_code   latitude  longitude  stars  review_count  is_open  \\\n",
       "2353         M2N  43.779041 -79.415528    4.0            26        0   \n",
       "\n",
       "                                             attributes  \\\n",
       "2353  {\"RestaurantsPriceRange2\": \"2\", \"GoodForKids\":...   \n",
       "\n",
       "                                             categories  \\\n",
       "2353  Tapas/Small Plates, Tapas Bars, Nightlife, Res...   \n",
       "\n",
       "                                                  hours  \n",
       "2353  {\"Monday\": \"17:30-2:30\", \"Tuesday\": \"17:30-2:3...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitor_direct_2nd_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In truth, after performing some investigations on the dataset, based on the address, the location & the types of categories that the restaurant used to approach, we have discovered that the restaurant has most likely __been rebranded__ into Hashi Izakaya, a level 3 competitor with our client. \n",
    "\n",
    "The change in competition level means that the restaurant has removed one of the similarities in the cuisine style that it used to approach customers with. Looking below at the comparison between the old and the new categories, we can actually see that it removed `Tapas/Small Plates`, which means that the restaurant now focuses on larger meals and full courses. Also, one more important thing to notice is that the new installment of this restaurant has added `Korean` to its list of cuisines, expanding its options and trying to reach for a broader audience. Indeed, it seems most likely that the change of name could have been done in order to create a new brand image of a mixed-Asian food restaurant, rather than just a Japanese-style one.\n",
    "\n",
    "Still, with 5 out of 7 categories shared between our client and this restaurant, it is still considered as a strong direct competitor, as it attacks most of the same pool of customers as our client does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2353</td>\n",
       "      <td>996axDvF9P8zxB9MJA52Xg</td>\n",
       "      <td>Yuzu Izakaya</td>\n",
       "      <td>5582 Yonge Street</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M2N</td>\n",
       "      <td>43.779041</td>\n",
       "      <td>-79.415528</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"RestaurantsPriceRange2\": \"2\", \"GoodForKids\":...</td>\n",
       "      <td>Tapas/Small Plates, Tapas Bars, Nightlife, Res...</td>\n",
       "      <td>{\"Monday\": \"17:30-2:30\", \"Tuesday\": \"17:30-2:3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4567</td>\n",
       "      <td>KxcQs2Lkm3FJiltVWXOz_Q</td>\n",
       "      <td>Hashi Izakaya</td>\n",
       "      <td>5582 Yonge Street</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M2N 5S2</td>\n",
       "      <td>43.779256</td>\n",
       "      <td>-79.415713</td>\n",
       "      <td>3.5</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"Smoking\": \"no\", \"Ambience\": {\"touristy\": \"Fa...</td>\n",
       "      <td>Korean, Tapas Bars, Japanese, Restaurants, Bar...</td>\n",
       "      <td>{\"Monday\": \"17:00-2:30\", \"Tuesday\": \"17:00-2:3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id           name            address     city state  \\\n",
       "2353  996axDvF9P8zxB9MJA52Xg   Yuzu Izakaya  5582 Yonge Street  Toronto    ON   \n",
       "4567  KxcQs2Lkm3FJiltVWXOz_Q  Hashi Izakaya  5582 Yonge Street  Toronto    ON   \n",
       "\n",
       "     postal_code   latitude  longitude  stars  review_count  is_open  \\\n",
       "2353         M2N  43.779041 -79.415528    4.0            26        0   \n",
       "4567     M2N 5S2  43.779256 -79.415713    3.5            61        1   \n",
       "\n",
       "                                             attributes  \\\n",
       "2353  {\"RestaurantsPriceRange2\": \"2\", \"GoodForKids\":...   \n",
       "4567  {\"Smoking\": \"no\", \"Ambience\": {\"touristy\": \"Fa...   \n",
       "\n",
       "                                             categories  \\\n",
       "2353  Tapas/Small Plates, Tapas Bars, Nightlife, Res...   \n",
       "4567  Korean, Tapas Bars, Japanese, Restaurants, Bar...   \n",
       "\n",
       "                                                  hours  \n",
       "2353  {\"Monday\": \"17:30-2:30\", \"Tuesday\": \"17:30-2:3...  \n",
       "4567  {\"Monday\": \"17:00-2:30\", \"Tuesday\": \"17:00-2:3...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toronto_all_business_df[toronto_all_business_df['name'].isin(['Yuzu Izakaya', 'Hashi Izakaya'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2353: 'Tapas/Small Plates, Tapas Bars, Nightlife, Restaurants, Bars, Japanese',\n",
       " 4567: 'Korean, Tapas Bars, Japanese, Restaurants, Bars, Nightlife'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toronto_all_business_df[toronto_all_business_df['name'].isin(['Yuzu Izakaya', 'Hashi Izakaya'])].categories.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2353: '{\"Monday\": \"17:30-2:30\", \"Tuesday\": \"17:30-2:30\", \"Wednesday\": \"17:30-2:30\", \"Thursday\": \"17:30-2:30\", \"Friday\": \"17:30-2:30\", \"Saturday\": \"17:30-2:30\", \"Sunday\": \"17:30-2:30\"}',\n",
       " 4567: '{\"Monday\": \"17:00-2:30\", \"Tuesday\": \"17:00-2:30\", \"Wednesday\": \"17:00-2:30\", \"Thursday\": \"17:00-2:30\", \"Friday\": \"17:00-2:30\", \"Saturday\": \"17:00-2:30\", \"Sunday\": \"17:00-2:30\"}'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toronto_all_business_df[toronto_all_business_df['name'].isin(['Yuzu Izakaya', 'Hashi Izakaya'])].hours.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the third level of competition, we see that we have 32 other businesses with similar market pool as the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 32 restaurants with 5/7 common categories as the client.\n"
     ]
    }
   ],
   "source": [
    "print('There are', competitor_direct_3rd_level.shape[0], 'restaurants with 5/7 common categories as the client.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly here, we will also join the set of competitors found earlier into one common dataframe, in order to have all the competitors aggregated in one structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor_df = competitor_direct_1st_level.append(competitor_direct_2nd_level).append(competitor_direct_3rd_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitor_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Restaurant competitors within the same neigbourhood/the near vicinity of the client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a further lookup and analysis of our client on Google Maps, we can see that the client is situated in the busy Willowdale area of North Toronto, on a restaurant-packed street situated next to a commercial center. Therefore, it seems reasonable to consider the area in which our client is located for finding more potential competitors to its business, regardless of the type of restaurant involved.\n",
    "\n",
    "We make the argument that, on a busy shopping street, people can often decide to try out new experiences and swap one restaurant for another, since there is easy proximity for the customers to choose any nearby restaurant at any moment. Therefore, we will consider competitors to be **any kind of Japanese restaurant business** situated within the same vicinity as our client.\n",
    "\n",
    "___Here, we define vicinity loosely based on the size of the neighbourhood in which the client is located. Since Willowdale is approximately 3 kilometers in radius (6 kilometers in diameters from one side to the other), we will consider any restaurant within 3 kilometers of the exact location of Kinka Izakaya to be a competitor.___ \n",
    "\n",
    "This will be measured using the Vincenty ellipsoidal distance between two sets of latitude/longitude coordinate points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_proximity_km(row1, row2):\n",
    "    # Let us create a coordinate tuple for the client\n",
    "    coord_tuple1 = (row1.latitude, row1.longitude)\n",
    "    \n",
    "    # We create a coordinate tuple for the business\n",
    "    coord_tuple2 = (row2.latitude, row2.longitude)\n",
    "    \n",
    "    return geo_distance.distance(coord_tuple1, coord_tuple2).kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance as geo_distance\n",
    "\n",
    "competitor_proximity_1km_df = pd.DataFrame(columns = competitor_df.columns)\n",
    "competitor_proximity_2km_df = pd.DataFrame(columns = competitor_df.columns)\n",
    "competitor_proximity_3km_df = pd.DataFrame(columns = competitor_df.columns)\n",
    "\n",
    "\n",
    "for _, business_row in toronto_japanese_business_df.iterrows():\n",
    "    \n",
    "    # Here, the distance in km between the business and the client is computed\n",
    "    distance_to_client = compute_proximity_km(client_row, business_row)\n",
    "    \n",
    "    # If the computed distance is < 1km, then add it to a proximity dataframe\n",
    "    if (distance_to_client <= 1):\n",
    "        competitor_proximity_1km_df = competitor_proximity_1km_df.append(business_row)\n",
    "        \n",
    "    # Or if the computed distance is between 1 and 2 km, then add it to a proximity dataframe\n",
    "    elif(distance_to_client > 1 and distance_to_client <= 2):\n",
    "        competitor_proximity_2km_df = competitor_proximity_2km_df.append(business_row)\n",
    "        \n",
    "    # Or if the computed distance is between 2 and 3 km, then add it to a proximity dataframe\n",
    "    elif(distance_to_client > 2 and distance_to_client <= 3):\n",
    "        competitor_proximity_3km_df = competitor_proximity_3km_df.append(business_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor_proximity_1km_df = competitor_proximity_1km_df[competitor_proximity_1km_df['name'].isin(kinka_all_branches_df['name'].values) == False]\n",
    "competitor_proximity_2km_df = competitor_proximity_2km_df[competitor_proximity_2km_df['name'].isin(kinka_all_branches_df['name'].values) == False]\n",
    "competitor_proximity_3km_df = competitor_proximity_3km_df[competitor_proximity_3km_df['name'].isin(kinka_all_branches_df['name'].values) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's also, separately, append these competitors into the joint `competitor_df` and remove potential duplicates, for future use in predictive modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of proximity competitors located within 1km is: 90\n",
      "The number of proximity competitors located between 1km and 2km is: 146\n",
      "The number of proximity competitors located between 2km and 3km is: 68\n"
     ]
    }
   ],
   "source": [
    "print('The number of proximity competitors located within 1km is:', competitor_proximity_1km_df.shape[0])\n",
    "print('The number of proximity competitors located between 1km and 2km is:',competitor_proximity_2km_df.shape[0])\n",
    "print('The number of proximity competitors located between 2km and 3km is:',competitor_proximity_3km_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor_df = competitor_df.append(competitor_proximity_1km_df).append(competitor_proximity_2km_df).append(competitor_proximity_3km_df)\n",
    "competitor_df = competitor_df[competitor_df.duplicated() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitor_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Restaurant competitors with similar overall popularity as our client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popularity is overall a difficult term to define, however it is essential to understand that a large-scale, business-class restaurant and a family-owned small-scale restaurant do not compete on the same terms, even if they tackle a similar customer market.\n",
    "\n",
    "Therefore, for this analysis, we will consider popularity to be a metric that is measured by __the product between the overall rating of the business and the number of total reviews given to the business (i.e. $nr_{reviews} \\cdot rating$)__. Since both of these variables indicate a positive aspect as they increase ascendingly, computing such a product will tell us that the higher this value is, the more is the business well-regarded across a wider variety of customers. The number of reviews alone is not enough, because perhaps a restaurant is well-known for its negative aspects or people have a strong opinion about it; the overall rating alone is also not enough, because this can be easily influenced statistically by the law of large numbers. Therefore, it is much easier to get a high rating when there are only very few ratings, but that is not conclusive. However, if there is a high score given by an equivalently high number of population, then that rating is rather stable and indicates good quality of the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_popularity_score(row):\n",
    "    return row.review_count * row.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6368.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_popularity_score(client_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can very clearly see that our client has an extremely high popularity score, computed according to the formula described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_id                                r_BrIgzYcwo1NAuG9dLbpg\n",
      "name                                    Pai Northern Thai Kitchen\n",
      "address                                          18 Duncan Street\n",
      "city                                                      Toronto\n",
      "state                                                          ON\n",
      "postal_code                                               M5H 3G8\n",
      "latitude                                                  43.6479\n",
      "longitude                                                -79.3887\n",
      "stars                                                         4.5\n",
      "review_count                                                 2758\n",
      "is_open                                                         1\n",
      "attributes      {\"RestaurantsTableService\": \"True\", \"BikeParki...\n",
      "categories      Restaurants, Thai, Specialty Food, Food, Ethni...\n",
      "hours           {\"Monday\": \"11:30-22:00\", \"Tuesday\": \"11:30-22...\n",
      "Name: 6288, dtype: object\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "i = 0 # This is the number of businesses (of any kind) in Toronto with higher popularity than our client\n",
    "\n",
    "for _, business_row in toronto_all_business_df.iterrows():\n",
    "    if (compute_popularity_score(business_row) > compute_popularity_score(client_row)):\n",
    "        i += 1\n",
    "        print(business_row)\n",
    "        print()\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, in the whole city of Toronto, our client is the most popular out of all Japanese restaurants, and is 2nd in popularity across all businesses. The only single business which is more popular than our client is _Pai Northern Thai Kitchen_, which is a restaurant with Thai cuisine.\n",
    "\n",
    "To see how large is the popularity of our client compared to the other Japanese restaurants, let us look at the average popularity score across Japanese restaurants in Toronto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303.1756097560976"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(\n",
    "    toronto_japanese_business_df\n",
    "    .apply(lambda business_row: compute_popularity_score(business_row), \n",
    "           axis = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average is ~300, and our client's score is ~6300, which is __21 times higher__ than the average. Clearly, at the time of the collection of dataset, the client was achieving very good scores from a large amount of population, however perhaps there is still something to learn from other Japanese restaurants with slightly lower popularity.\n",
    "\n",
    "Since the scoring average is 300, we will consider any Japanese restaurant __with a popularity score of over 1000__ to be a popularity competitor with our client, since this means that, on average, the restaurant must have had e.g. more than 250 reviews of 4.0 stars, or an equivalent number, in order to achieve this treshhold. The number 1000 was selected since most of the Japanese restaurant businesses in Toronto are smaller-scale, family restaurants (according to the score calculation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor_popularity_df = pd.DataFrame(columns = competitor_df.columns)\n",
    "\n",
    "\n",
    "for _, business_row in toronto_japanese_business_df.iterrows():\n",
    "    \n",
    "    # If the popularity score is higher than 1000, then add that business as a competitor\n",
    "    if (compute_popularity_score(business_row) > 1000):\n",
    "        competitor_popularity_df = competitor_popularity_df.append(business_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor_popularity_df = competitor_popularity_df[competitor_popularity_df['name'].isin(kinka_all_branches_df['name'].values) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitor_popularity_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's also, separately, append these competitors into the joint `competitor_df` and remove potential duplicates, for future use in predictive modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor_df = competitor_df.append(competitor_popularity_df)\n",
    "competitor_df = competitor_df[competitor_df.duplicated() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitor_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have finalized our competitor search, however, it is important to understand that not all competitors are equally important for the business, and there are some characteristics, such as tackling the same cuisine categories, which make competition fierce, while others, such as simple longer-distance proximity, which are of lesser importance (while still being important).\n",
    "\n",
    "Therefore, it is important to be able to identify and classify competitors across the unified dataframe (`competitor_df`), in order for our analysis to be more consistent and feasible. At the same time, a classification of this sort will also help our prediction model later on to identify potential increased number of customers from adjusting opening hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the business sense with which we have selected our competitors here, we will assign a `competitor_importance` variable, which will go from 0 to 7 in the following way:\n",
    "\n",
    "* **7: Any direct competitors of the 1st level, with 7/7 client categories matching**\n",
    "* **6: Any direct competitors of the 2nd level, with 6/7 client categories matching**\n",
    "* **5: Any direct competitors of the 3rd level, with 5/7 client categories matching**\n",
    "* **4: Any competitors with similar popularity score as our client (ie. > 1000)**\n",
    "* **3: Any proximity competitors located within 1km radius of our client's business location**\n",
    "* **2: Any proximity competitors located within a radius of between 1 and 2 km of our client's business location**\n",
    "* **1: Any proximity competitors located within a radius of between 2 and 3 km of our client's business location**\n",
    "* **0: Any businesses which are in neither of the competitor sections above, therefore are not direct or indirect competitors to our client**\n",
    "\n",
    "If a competitor fits inside multiple levels, the highest level will be recorded. We have chosen not to add the levels together (for a competitor which can fit in multiple of these categories), because that will destroy the interpretability of the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_competitor_importance(row):\n",
    "    importance = 0\n",
    "    \n",
    "    # We do not use elif constructs here, since if a competitor fits in multiple importance levels, \n",
    "    # we want to overwrite the importance to be the highest value possible\n",
    "    if (row.business_id in competitor_proximity_3km_df.business_id.to_list()):\n",
    "        importance = 1\n",
    "    if (row.business_id in competitor_proximity_2km_df.business_id.to_list()):\n",
    "        importance = 2\n",
    "    if (row.business_id in competitor_proximity_1km_df.business_id.to_list()):\n",
    "        importance = 3\n",
    "    if (row.business_id in competitor_popularity_df.business_id.to_list()):\n",
    "        importance = 4\n",
    "    if (row.business_id in competitor_direct_3rd_level.business_id.to_list()):\n",
    "        importance = 5\n",
    "    if (row.business_id in competitor_direct_2nd_level.business_id.to_list()):\n",
    "        importance = 6\n",
    "    if (row.business_id in competitor_direct_1st_level.business_id.to_list()):\n",
    "        importance = 7\n",
    "        \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor_df['competitor_importance'] = competitor_df.apply (\n",
    "                                            lambda business_row: compute_competitor_importance(business_row),\n",
    "                                            axis = 1\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the distribution of the importance levels of different competitors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7      1\n",
      "6      1\n",
      "5     32\n",
      "4     28\n",
      "3     79\n",
      "2    134\n",
      "1     62\n",
      "Name: competitor_importance, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(competitor_df.competitor_importance.value_counts().sort_index(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems clear now that our competitor analysis is well-distributed, since the category with most competitors is the lower-level category where other Japanese restaurants are in a close proximity to our client, and as we increase the importance level up towards 7, the number of competitors decreases rapidly. This is normal, since the higher levels indicate a more fierce and direct competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of all the 615 Japanese restaurants in Toronto, our client, KINKA IZAKAYA ORIGINAL, is competing directly or indirectly with 337 of them.\n",
      "This represents 54.8% of the total number of Japanese restaurants.\n"
     ]
    }
   ],
   "source": [
    "print('Out of all the %d Japanese restaurants in Toronto, our client, %s, is competing directly or indirectly with %d of them.\\nThis represents %.1f%% of the total number of Japanese restaurants.' \n",
    "      % (toronto_japanese_business_df.shape[0], \n",
    "         client_row['name'], \n",
    "         competitor_df.shape[0],\n",
    "         competitor_df.shape[0] / toronto_japanese_business_df.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Sentiment Analysis & Text Analytics upon Client vs. Competitor reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>mM8i91yWP1QbImEvz5ds0w</td>\n",
       "      <td>TZQSUDDcA4ek5gBd6BzcjA</td>\n",
       "      <td>qUWqjjjfpB2-4P3He5rsKw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In the heart of Chinatown, I discovered it enr...</td>\n",
       "      <td>2017-06-06 19:04:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>nAm92_-WUQ084x9Uz9iFmA</td>\n",
       "      <td>iFEM16O5BeKvf_AHs6NzUQ</td>\n",
       "      <td>9Eghhu_LzEJgDKNgisf3rg</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Was super excited to bring my boyfriend to thi...</td>\n",
       "      <td>2016-08-25 16:25:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>B_1HqrwINKkIEnekkxtpsA</td>\n",
       "      <td>1kNsEAhGU8d8xugMuXJGFA</td>\n",
       "      <td>cDoo0Pf1d_P79Rq3ZNk-Mw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sigh. This review was tough to give, as it cou...</td>\n",
       "      <td>2013-07-13 00:57:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  mM8i91yWP1QbImEvz5ds0w  TZQSUDDcA4ek5gBd6BzcjA  qUWqjjjfpB2-4P3He5rsKw   \n",
       "1  nAm92_-WUQ084x9Uz9iFmA  iFEM16O5BeKvf_AHs6NzUQ  9Eghhu_LzEJgDKNgisf3rg   \n",
       "2  B_1HqrwINKkIEnekkxtpsA  1kNsEAhGU8d8xugMuXJGFA  cDoo0Pf1d_P79Rq3ZNk-Mw   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    4.0       0      0     0   \n",
       "1    3.0       0      0     0   \n",
       "2    3.0       1      0     0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  In the heart of Chinatown, I discovered it enr...  2017-06-06 19:04:10  \n",
       "1  Was super excited to bring my boyfriend to thi...  2016-08-25 16:25:12  \n",
       "2  Sigh. This review was tough to give, as it cou...  2013-07-13 00:57:45  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews=pd.read_csv('data/toronto_restaurant_reviews.csv')\n",
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are only intressed in the stars, as they provide information about the sentiment of the reviews, the business_id, connecting the reviews to the cooresponding business and the the review it self. All other columns are therefore droped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=reviews.drop(['review_id','user_id','useful','funny','cool','date'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore we are only intressted in reviews belonging either to our client or one of their compettitors, and reviews for any other resturant is therefore removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=reviews[(reviews.business_id.isin(list(competitor_df.business_id))) | (reviews.business_id==client_row[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any analysis is carried out the reviews are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\clara\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Removing punctuation\n",
    "reviews.text=reviews.text.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "#transfer to lower case:\n",
    "reviews.text=reviews.text.apply(lambda x: x.lower())\n",
    "#Removing stopwords\n",
    "reviews.text=reviews.text.apply(lambda x: ' '.join([word for word in x.split() if word not in list(stop_words.ENGLISH_STOP_WORDS)]))\n",
    "#Stemming or lemmatizing\n",
    "reviews.text=reviews.text.apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventhough stars are available along with the reviews, indicating the overall sentiment of the resturant, what different reviewers consider worthy of certain star rating are very subjective. Therefore we initially perform a an sentiment alanysis on the reviews. Then sentiment analysis build on the sentiment scores defined in AFINN-111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check if definition is taken from exercise notebook.\n",
    "import csv\n",
    "def build_sent_dictionary(sent_filename):\n",
    "    # initialize an empty dictionary\n",
    "    sent_dictionary = {} \n",
    "    afinnfile = open(sent_filename)\n",
    "    for line in afinnfile:\n",
    "            w, score=line.split(\"\\t\")\n",
    "            sent_dictionary[w]=int(score)\n",
    "    afinnfile.close()\n",
    "    return sent_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN=build_sent_dictionary(\"data/AFINN-111.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_sentiments(review, sent_dictionary):\n",
    "    words = [word for word in review.split()]\n",
    "    score=0\n",
    "    for word in words:\n",
    "        s=sent_dictionary.get(word)\n",
    "        if (s != None):\n",
    "            score+=sent_dictionary.get(word)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['sentiment_score']=reviews.text.apply(lambda l: rate_sentiments(l,AFINN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the only level 7 competitor, Ogadang is the most important competitor for our client, and their for the inital focus of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "      <th>competitor_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11730</td>\n",
       "      <td>igo--IP_NBWIO_yGtBJ0Eg</td>\n",
       "      <td>Ogadang</td>\n",
       "      <td>41 Spring Garden Avenue</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M2N 5N6</td>\n",
       "      <td>43.764552</td>\n",
       "      <td>-79.410009</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"Ambience\": {\"touristy\": \"False\", \"hipster\": ...</td>\n",
       "      <td>Restaurants, Japanese, Pubs, Tapas/Small Plate...</td>\n",
       "      <td>{\"Tuesday\": \"17:30-2:30\", \"Wednesday\": \"17:30-...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  business_id     name                  address     city  \\\n",
       "11730  igo--IP_NBWIO_yGtBJ0Eg  Ogadang  41 Spring Garden Avenue  Toronto   \n",
       "\n",
       "      state postal_code   latitude  longitude  stars review_count is_open  \\\n",
       "11730    ON     M2N 5N6  43.764552 -79.410009    4.0           18       1   \n",
       "\n",
       "                                              attributes  \\\n",
       "11730  {\"Ambience\": {\"touristy\": \"False\", \"hipster\": ...   \n",
       "\n",
       "                                              categories  \\\n",
       "11730  Restaurants, Japanese, Pubs, Tapas/Small Plate...   \n",
       "\n",
       "                                                   hours  \\\n",
       "11730  {\"Tuesday\": \"17:30-2:30\", \"Wednesday\": \"17:30-...   \n",
       "\n",
       "       competitor_importance  \n",
       "11730                      7  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitor_df[competitor_df.competitor_importance==7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesID_level7=list(competitor_df[competitor_df.competitor_importance==7].business_id)\n",
    "reviews_level7=reviews[reviews.business_id.isin(businesID_level7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sentiment scores, the reviews are now divided into negative and positve reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ogadang has 17 reviews with a positive sentiment score\n",
      "Ogadang has 0 reviews with a negative sentiment score\n"
     ]
    }
   ],
   "source": [
    "reviews_positive_level7=reviews_level7[reviews_level7.sentiment_score>0]\n",
    "reviews_negative_level7=reviews_level7[reviews_level7.sentiment_score<0]\n",
    "print(\"Ogadang has {} reviews with a positive sentiment score\".format(len(reviews_positive_level7)))\n",
    "print(\"Ogadang has {} reviews with a negative sentiment score\".format(len(reviews_negative_level7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that Ogadang has no reviews with negative sentiment scores, indicating that it is indeed a storng competiter for the client.<br>\n",
    "What we are now interested in is identifying the topics of the reviews, as this allow us to investigate what the customers consider positive about the resturant, and there fore which parameter it competes on.<br>\n",
    "\n",
    "Here we are no longer interested in words that are usually used to express a sentiment, and such words are therefore removed from the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sentiment_words(review):\n",
    "    review_without_sentiment  = [word for word in review.split() if word not in AFINN.keys()]\n",
    "    return' '.join(review_without_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_positive_level7.update(reviews_positive_level7.text.apply(lambda l: remove_sentiment_words(l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready transform the the reviews into a bag-of-words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "bow_l7_positive = count_vect.fit_transform(reviews_positive_level7['text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the reviews represented in a bag-of-words format, we are now ready to investigate the topics of both positve and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 5\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=15,\n",
    "                                random_state=0)\n",
    "\n",
    "IDA_7_positive=lda.fit(bow_l7_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "##At the moment the function is taken from exercises. If keept then a refference will need to be added. ##\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    norm = model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(80 * \"-\")\n",
    "        print(\"Topic {}\".format(topic_idx))\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            print(\"{:.3f}\".format(topic[i] / norm[topic_idx][0]) \n",
    "                  + '\\t' + feature_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model for positive reviews:\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 0\n",
      "0.018\tyakitori\n",
      "0.018\titem\n",
      "0.018\tim\n",
      "0.013\treally\n",
      "0.013\tarea\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 1\n",
      "0.015\tmenu\n",
      "0.015\tfood\n",
      "0.012\togadang\n",
      "0.012\tdrink\n",
      "0.012\tyakitori\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 2\n",
      "0.025\tfood\n",
      "0.014\tyakitori\n",
      "0.014\treally\n",
      "0.012\tcame\n",
      "0.012\tordered\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 3\n",
      "0.018\tskewer\n",
      "0.018\tfood\n",
      "0.015\tplace\n",
      "0.015\tstyle\n",
      "0.015\tbit\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 4\n",
      "0.027\tcheese\n",
      "0.027\tfried\n",
      "0.020\tchicken\n",
      "0.020\tbit\n",
      "0.020\tdonburi\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model for positive reviews:\")\n",
    "counts_feature_names = count_vect.get_feature_names()\n",
    "n_top_words = 5\n",
    "print_top_words(IDA_7_positive, counts_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the identified topics...\n",
    "**Would a wordcloud be better than looking at topic identification....**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now having looked at the topics that characterize our clients stronest competitor, indicating its competitive advantages these should be understood in relation to the client. Therefore a similar analysis is performed on the clients reviews. While the client have serveral local versions, only the original is here included as, there can be local differences between the differen braches of the resturant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_client=reviews[reviews.business_id==client_row[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ogadang has 1528 reviews with a positive sentiment score\n",
      "Ogadang has 55 reviews with a negative sentiment score\n"
     ]
    }
   ],
   "source": [
    "reviews_positive_client=reviews_client[reviews_client.sentiment_score>0]\n",
    "reviews_negative_client=reviews_client[reviews_client.sentiment_score<0]\n",
    "print(\"Ogadang has {} reviews with a positive sentiment score\".format(len(reviews_positive_client)))\n",
    "print(\"Ogadang has {} reviews with a negative sentiment score\".format(len(reviews_negative_client)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the majority of reviews are positive, a few reviews also have negative sentiment score. We start by analysing the initial  positive reviews, to discover the strengths of the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_positive_client.update(reviews_positive_client.text.apply(lambda l: remove_sentiment_words(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "bow_client_positive = count_vect.fit_transform(reviews_positive_client['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 3\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=15,\n",
    "                                random_state=0)\n",
    "\n",
    "IDA_client_positive=lda.fit(bow_client_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model for negative reviews:\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 0\n",
      "0.015\tdish\n",
      "0.013\tfried\n",
      "0.010\tfood\n",
      "0.008\tplace\n",
      "0.008\ttime\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 1\n",
      "0.021\tfood\n",
      "0.019\tplace\n",
      "0.011\tservice\n",
      "0.008\tsake\n",
      "0.007\ttime\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 2\n",
      "0.030\tfood\n",
      "0.023\tplace\n",
      "0.013\tjapanese\n",
      "0.011\ttime\n",
      "0.011\treally\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model for positive reviews:\")\n",
    "counts_feature_names = count_vect.get_feature_names()\n",
    "n_top_words = 5\n",
    "print_top_words(IDA_client_positive, counts_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is less clear to identify any generel topics for the clients positive revies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the negative reviews for the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_negative_client.update(reviews_negative_client.text.apply(lambda l: remove_sentiment_words(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "bow_client_negative = count_vect.fit_transform(reviews_negative_client['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=15,\n",
    "                                random_state=0)\n",
    "\n",
    "IDA_client_negative=lda.fit(bow_client_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model for negative reviews:\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 0\n",
      "0.012\tvous\n",
      "0.012\tle\n",
      "0.010\tfood\n",
      "0.009\tÃ§a\n",
      "0.009\tune\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 1\n",
      "0.021\tplace\n",
      "0.016\tfood\n",
      "0.012\tservice\n",
      "0.010\tjust\n",
      "0.009\ttoronto\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 2\n",
      "0.017\tfood\n",
      "0.011\tordered\n",
      "0.011\tloud\n",
      "0.009\trestaurant\n",
      "0.009\tjapanese\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 3\n",
      "0.018\tfood\n",
      "0.013\tloud\n",
      "0.013\tservice\n",
      "0.011\tplace\n",
      "0.010\tatmosphere\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model for negative reviews:\")\n",
    "counts_feature_names = count_vect.get_feature_names()\n",
    "n_top_words = 5\n",
    "print_top_words(IDA_client_negative, counts_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the negative reviews for our client it the topics are more meningful. With topic 1 focusing on the service, topic 1 on the ordered food and topic 3 on a noice level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the only competitor with an imprtance score of 6, Yuzu Izakaya also represent an important comptiotor for our client. As similar analysis is therefore also carried out for Yuzu Izakaya to investigate copetetive parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "      <th>competitor_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2353</td>\n",
       "      <td>996axDvF9P8zxB9MJA52Xg</td>\n",
       "      <td>Yuzu Izakaya</td>\n",
       "      <td>5582 Yonge Street</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>M2N</td>\n",
       "      <td>43.779041</td>\n",
       "      <td>-79.415528</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"RestaurantsPriceRange2\": \"2\", \"GoodForKids\":...</td>\n",
       "      <td>Tapas/Small Plates, Tapas Bars, Nightlife, Res...</td>\n",
       "      <td>{\"Monday\": \"17:30-2:30\", \"Tuesday\": \"17:30-2:3...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id          name            address     city state  \\\n",
       "2353  996axDvF9P8zxB9MJA52Xg  Yuzu Izakaya  5582 Yonge Street  Toronto    ON   \n",
       "\n",
       "     postal_code   latitude  longitude  stars review_count is_open  \\\n",
       "2353         M2N  43.779041 -79.415528    4.0           26       0   \n",
       "\n",
       "                                             attributes  \\\n",
       "2353  {\"RestaurantsPriceRange2\": \"2\", \"GoodForKids\":...   \n",
       "\n",
       "                                             categories  \\\n",
       "2353  Tapas/Small Plates, Tapas Bars, Nightlife, Res...   \n",
       "\n",
       "                                                  hours  competitor_importance  \n",
       "2353  {\"Monday\": \"17:30-2:30\", \"Tuesday\": \"17:30-2:3...                      6  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitor_df[competitor_df.competitor_importance==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesID_level6=list(competitor_df[competitor_df.competitor_importance==6].business_id)\n",
    "reviews_level6=reviews[reviews.business_id.isin(businesID_level6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yuzu Izakaya has 26 reviews with a positive sentiment score\n",
      "Yuzu Izakaya has 1 reviews with a negative sentiment score\n"
     ]
    }
   ],
   "source": [
    "reviews_positive_level6=reviews_level6[reviews_level6.sentiment_score>0]\n",
    "reviews_negative_level6=reviews_level6[reviews_level6.sentiment_score<0]\n",
    "print(\"Yuzu Izakaya has {} reviews with a positive sentiment score\".format(len(reviews_positive_level6)))\n",
    "print(\"Yuzu Izakaya has {} reviews with a negative sentiment score\".format(len(reviews_negative_level6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only one negative review a more in depth text analysis is not meningful. However, when taking a look at the only negative review below, after it has been through the initial normalization, it appear that the review found Yuzu Izakaya to be overpriced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46804    cheated nome came pricing bit hefty izakaya en...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_negative_level6.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the positive reviews it is still posible to look for overall topics, to try and identify strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_positive_level6.update(reviews_positive_level6.text.apply(lambda l: remove_sentiment_words(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "bow_level6_positive = count_vect.fit_transform(reviews_positive_level6['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=15,\n",
    "                                random_state=0)\n",
    "\n",
    "IDA_level6_positive=lda.fit(bow_level6_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model for positive reviews:\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 0\n",
      "0.031\tizakaya\n",
      "0.019\tfood\n",
      "0.012\tfried\n",
      "0.012\tmenu\n",
      "0.011\tsalad\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 1\n",
      "0.016\tcome\n",
      "0.015\tsalmon\n",
      "0.013\ttime\n",
      "0.011\torder\n",
      "0.011\tsauce\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 2\n",
      "0.012\treally\n",
      "0.012\tdish\n",
      "0.010\tportion\n",
      "0.010\tsoup\n",
      "0.010\tskewer\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 3\n",
      "0.019\tplace\n",
      "0.016\tdrink\n",
      "0.014\tizakaya\n",
      "0.013\tfood\n",
      "0.012\treally\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model for positive reviews:\")\n",
    "counts_feature_names = count_vect.get_feature_names()\n",
    "n_top_words = 5\n",
    "print_top_words(IDA_level6_positive, counts_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we see the food also appear to be the topic of the positive reviews for Yuzu Izakaya, salad and soup are also being mentioned here, which is something that have not been mentioed for the resturants wchich was investigated previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32 competitors was identified for the client, with an importance of 5. These are still important for the client to be aware off, but on a more general level compared to the level 6 and 7 competitors, which we previously looked into. Therefore we are here going to try and identify general characteristics for what the these resturants are doing well, and also what challenges they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(competitor_df[competitor_df.competitor_importance==5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesID_level5=list(competitor_df[competitor_df.competitor_importance==5].business_id)\n",
    "reviews_level5=reviews[reviews.business_id.isin(businesID_level5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For competitors of importance 5 there are 2742 reviews with a positive sentiment score\n",
      "For competitors of importance 5 there are 195 reviews with a negative sentiment score\n"
     ]
    }
   ],
   "source": [
    "reviews_positive_level5=reviews_level5[reviews_level5.sentiment_score>0]\n",
    "reviews_negative_level5=reviews_level5[reviews_level5.sentiment_score<0]\n",
    "print(\"For competitors of importance 5 there are {} reviews with a positive sentiment score\".format(len(reviews_positive_level5)))\n",
    "print(\"For competitors of importance 5 there are {} reviews with a negative sentiment score\".format(len(reviews_negative_level5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by looking into the positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_positive_level5.update(reviews_positive_level5.text.apply(lambda l: remove_sentiment_words(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "bow_level5_positive = count_vect.fit_transform(reviews_positive_level5['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=15,\n",
    "                                random_state=0)\n",
    "\n",
    "IDA_level5_positive=lda.fit(bow_level5_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model for positive reviews:\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 0\n",
      "0.013\tfood\n",
      "0.011\tplace\n",
      "0.010\treally\n",
      "0.009\tordered\n",
      "0.009\tdish\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 1\n",
      "0.022\tfood\n",
      "0.018\tplace\n",
      "0.016\toyster\n",
      "0.012\ttime\n",
      "0.011\tservice\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 2\n",
      "0.019\ttaco\n",
      "0.014\tfry\n",
      "0.012\tfood\n",
      "0.011\tkimchi\n",
      "0.010\tdrink\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 3\n",
      "0.016\tbar\n",
      "0.015\tplace\n",
      "0.013\tfood\n",
      "0.010\tdrink\n",
      "0.009\twine\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model for positive reviews:\")\n",
    "counts_feature_names = count_vect.get_feature_names()\n",
    "n_top_words = 5\n",
    "print_top_words(IDA_level5_positive, counts_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that topic 3 appear to focus on alcohol, while topic two appear to be focus on international cuisine, with taco and kimchi being mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now gonna take a look at the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\clara\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\frame.py:5804: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = expressions.where(mask, this, that)\n"
     ]
    }
   ],
   "source": [
    "reviews_negative_level5.update(reviews_negative_level5.text.apply(lambda l: remove_sentiment_words(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "bow_level5_negative = count_vect.fit_transform(reviews_negative_level5['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 2\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=15,\n",
    "                                random_state=0)\n",
    "\n",
    "IDA_level5_negative=lda.fit(bow_level5_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model for negative reviews:\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 0\n",
      "0.013\tfood\n",
      "0.009\ttime\n",
      "0.008\tplace\n",
      "0.008\tjust\n",
      "0.007\toyster\n",
      "--------------------------------------------------------------------------------\n",
      "Topic 1\n",
      "0.018\tfood\n",
      "0.014\tservice\n",
      "0.010\trestaurant\n",
      "0.009\tplace\n",
      "0.008\ttime\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model for negative reviews:\")\n",
    "counts_feature_names = count_vect.get_feature_names()\n",
    "n_top_words = 5\n",
    "print_top_words(IDA_level5_negative, counts_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appear that over all the negative reviews are about the service of the resturants, or the time. While the general topic  of food not suprisingly appear to be part of all the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Prediction of possible customer increase by increasing opening hours\n",
    "\n",
    "### 1. Data Modeling & Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our client is distinguishing itself from many evening-open restaurants through its unique Japanese concept, the hours of its operation are placing it in a heavily competitive situation, since the vast majority of restaurants (of any kind) is open during the evening hours of 17:00 to 00:00, since that is when most people are off-work and able to enjoy their own leisure time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_row.hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are many restaurant businesses to choose from during those hours, which can disperse out the mass of customers across the many restaurant that are both open and in short vicinity to our client. There are many ways in which our client could change its business operation to capture more customers, and, ___as a key point, it may be insightful to look into what kind of effect would increased opening hours have upon customers influx.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, we can see that our client opens the business operations at __17.00__, with undergoing preparations for dinner servings probably taking place starting 1-2 hours before that. The opening hours of our client must be taken into consideration with respect to a KPI suggesting that the flux of customers available per hour which is enough for breaking-even or profit-making starts at 17.00. However, we are interested in studying the customer influx from other restaurants during earlier hours of operation, and come up with a predicition of whether it is a profitable idea for our client to start the business operations earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In this business analysis, we are going to create and provide a prediction model which investgates the total proportion of customers per weekday that our client can recieve by increasing their business hours and starting their operations one hour earlier, namely, at 16:00.__\n",
    "\n",
    "For this predictive model, we will need to make some strict assumptions related to the data available, we will need to model the data to a high degree to prepare it for ingestion into a prediction model, and finally, we will use __Deep Learning__ to perform a _regression task_ upon the customer influx. All of these steps will be detailed clearly below, as we progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we delve into the steps necessary for proper data modelling and augmentation to achieve predictions, it is essential that we discuss the assumptions upon which the model will be built on:\n",
    "\n",
    "* We are trying to predict a measure of customer influx between a specific time-frame (in our case, between 16:00 and 17:00). This means that, to achieve this task, we need data about previous customer influx in the restaurants of Toronto. There is no specific dataset which tackles this problem, however ___Yelp has provided us with a list of checkins which have been done through their platform.___\n",
    "\n",
    "* These checkins are a measure of customer influx, however there are by far much fewer checkins recorded through the Yelp service than there are actual customers in a day. At the same time, the checkins span the whole timespan of the restaurant, from the opening day up until the current date. __Therefore, we will make the assumption that the total number of checkins distributed across each _weekday_ in part (e.g. Monday, Tuesday, etc.) is _directly proportional_ with the actual total number of customers that have visited the shop.__\n",
    "\n",
    "* Therefore, it is clear that we do not have the necessary data to predict an exact number of customers that might visit the restaurant between certain hours, since this data has not been recorded. __However, using the assumption above, we can aggregate the total number of checkins per each weekday, and divide the total number of checkins between a certain hour in a certain weekday, (e.g. Monday) to the total number of checkins during such a weekday (e.g. the total number of all checkins made on a Monday).__ This will give us a proportion that indicates what percentage of customers on a weekday come between the specific hours (out of all the customers which have come on that type of weekday, at any hour).\n",
    "\n",
    "* __In order for the above assumption to work, we assume that every weekday will always keep the same customer influx pattern, and therefore, we disregard time-series seasonality patters and trend differences.__ This is because, if the checkin data is not aggregated according to each weekday, there is not enough data to find the seasonality trends from the checkins alone, since the checkin data is very sparse, due to its nature of being a completely voluntary action within the Yelp ecosystem.\n",
    "\n",
    "* One final consideration that we do address in our modeling is that: each restaurant may have different opening hours for each weekday. Therefore, __the customer proportion which comes during certain hours will of course be dependent on the opening hours of the restaurant. That is why we will separate and consider the checkin data for each weekday in part, and we will compute a different customer proportion depending each of the 7 weekdays, for every restaurant, in order to account for such differences.__\n",
    "\n",
    "Therefore, once again, our model will not be able to provide an exact number of customers that would be visiting the client during increased opening hours, instead it will output the proportion of customers that would visit our client during the specified extra hour of opening time, out of an aggregated number of all the customers that visit the izakaya during such a weekday. We will then compute how many customers that would approximately represent, based on the total aggregated number of customers for each weekday in part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us define the dataset that we are working with: for the prediction deep learning model, we will need as much relevant data as possible about the customer influx across the _Japanese restaurants_ of Toronto. That is why, for this task, we will include and work with any of the Japanese restaurants that we have available in our data (not only the direct/indirect competitors described earlier in the notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data = toronto_japanese_business_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, competitors will be distinguished through applying upon this Toronto data the same data modeling steps as for the competitor dataframe: an importance score, a proximity calculation and a popularity score. Japanese restaurants which have not been featured previously as competitors will be present in this dataset with an importance score of 0 (as programmed earlier as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Augmentation with business importance, proximity and popularity scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data['importance'] = augmented_japanese_data.apply(\n",
    "                                            lambda business_row: compute_competitor_importance(business_row), axis = 1\n",
    "                                        )\n",
    "\n",
    "augmented_japanese_data['proximity'] = augmented_japanese_data.apply(\n",
    "                                            lambda business_row: compute_proximity_km(business_row, client_row), axis = 1\n",
    "                                       )\n",
    "\n",
    "augmented_japanese_data['popularity'] = augmented_japanese_data.apply(\n",
    "                                            lambda business_row: compute_popularity_score(business_row), axis = 1\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of Japanese restaurants in Toronto, all of which will be used for prediction modeling, is:', augmented_japanese_data.shape[0] - 1) # excluding our client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to know the total amount of data available to us for this challenging task, and it looks like there are 614 restaurants (excluding our client) from which we can create training and testing data for our Neural Network regression model.\n",
    "\n",
    "Our current amount of variables is rather limited, since most of the variables in the dataset are related to general business features, such as name and address, which have no predictive power and will not be introduced in the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Augmentation with nearby amenities from OpenStreetMaps API - sources of customer influx:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Therefore, in order to increase the chances that we have relevant attributes in our dataset which can be good predictors of the proportion of customers coming between the hours of 16:00 and 17:00 for a business, we will use the power of _OpenStreetMaps_ in order to collect data about places of interest (POI) near and around the business on a radius of ~3 kilometers, a proximity which should provide clients easy access from these amenities.__\n",
    "\n",
    "We will use __Web Data Mining__ in order to extract data from an OpenStreetMaps API regarding the total number of amenities that one can find within a radius of 3km of each restarant in our dataset. ___In total, for each restaurant, we collect data about 20 amenities which we have carefully considered to be relevant for the influx of customers, among which we have the number of universities, libraries, arts centres, cafes, bars, other restaurants (of any kind), cinemas, and more.___ Our logic behind selecting these amenities is that: e.g. libraries and cinemas are strong sources of potential customers which are anyway in the proximity and might decide to stop by.\n",
    "\n",
    "We will use a radius of 0.011 units, which corresponds to approximatively 3.02 km as the circular area surrounding the businesses, within which we will perform our search for amenities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is a printed example of the JSON data output from the Overpass API:\n",
    "\n",
    "r = 0.011\n",
    "# Constructing the special String query\n",
    "obj = 'node[\"amenity\" = \"%s\"]'%('library')\n",
    "\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "overpass_query = \"\"\"\n",
    "                    [out:json];\n",
    "                    {0}({1},{2},{3},{4});\n",
    "                    out center;\n",
    "                 \"\"\".format(obj, client_row.latitude - r, client_row.longitude - r, client_row.latitude + r, client_row.longitude + r)\n",
    "\n",
    "# Use Python's RESTful API to retrieve data in json format, as it is written the query parameter\n",
    "response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "response_json = response.json()\n",
    "pprint(response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.011 # the radius is approximately 3.02 km from the circular center\n",
    "\n",
    "def find_amenity_count(amenity_str, lat, long):\n",
    "    \n",
    "    # Constructing the special String query\n",
    "    obj = 'node[\"amenity\" = \"%s\"]'%(amenity_str)\n",
    "    \n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    overpass_query = \"\"\"\n",
    "                        [out:json];\n",
    "                        {0}({1},{2},{3},{4});\n",
    "                        out center;\n",
    "                     \"\"\".format(obj, lat - r, long - r, lat + r, long + r)\n",
    "    \n",
    "    # Use Python's RESTful API to retrieve data in json format, as it is written the query parameter\n",
    "    response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "    response_json = response.json()\n",
    "    \n",
    "    return len(response_json['elements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_amenity_count_columns(row, row_index):\n",
    "    try:\n",
    "        row['osm_restaurant'] = find_amenity_count('restaurant', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_restaurant'] = 0\n",
    "#     print('Row ', row_index, ', Finished 1/20 amenities')\n",
    "    try:\n",
    "        row['osm_bar'] = find_amenity_count('bar', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_bar'] = 0\n",
    "#     print('Row ', row_index, ', Finished 2/20 amenities')\n",
    "    try:\n",
    "        row['osm_cafe'] = find_amenity_count('cafe', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_cafe'] = 0\n",
    "#     print('Row ', row_index, ', Finished 3/20 amenities')\n",
    "    try:\n",
    "        row['osm_fast_food'] = find_amenity_count('fast_food', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_fast_food'] = 0\n",
    "#     print('Row ', row_index, ', Finished 4/20 amenities')\n",
    "    try:\n",
    "        row['osm_college'] = find_amenity_count('college', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_college'] = 0\n",
    "#     print('Row ', row_index, ', Finished 5/20 amenities')\n",
    "    try:\n",
    "        row['osm_library'] = find_amenity_count('library', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_library'] = 0\n",
    "#     print('Row ', row_index, ', Finished 6/20 amenities')\n",
    "    try:\n",
    "        row['osm_school'] = find_amenity_count('school', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_school'] = 0\n",
    "#     print('Row ', row_index, ', Finished 7/20 amenities')\n",
    "    try:\n",
    "        row['osm_university'] = find_amenity_count('university', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_university'] = 0\n",
    "#     print('Row ', row_index, ', Finished 8/20 amenities')\n",
    "    try:\n",
    "        row['osm_bus_station'] = find_amenity_count('bus_station', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_bus_station'] = 0\n",
    "#     print('Row ', row_index, ', Finished 9/20 amenities')\n",
    "    try:\n",
    "        row['osm_parking'] = find_amenity_count('parking', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_parking'] = 0\n",
    "#     print('Row ', row_index, ', Finished 10/20 amenities')\n",
    "    try:\n",
    "        row['osm_taxi'] = find_amenity_count('taxi', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_taxi'] = 0\n",
    "#     print('Row ', row_index, ', Finished 11/20 amenities')\n",
    "    try:\n",
    "        row['osm_bank'] = find_amenity_count('bank', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_bank'] = 0\n",
    "#     print('Row ', row_index, ', Finished 12/20 amenities')\n",
    "    try:\n",
    "        row['osm_hospital'] = find_amenity_count('hospital', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_hospital'] = 0\n",
    "#     print('Row ', row_index, ', Finished 13/20 amenities')\n",
    "    try:\n",
    "        row['osm_social_facility'] = find_amenity_count('social_facility', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_social_facility'] = 0\n",
    "#     print('Row ', row_index, ', Finished 14/20 amenities')\n",
    "    try:\n",
    "        row['osm_arts_centre'] = find_amenity_count('arts_centre', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_arts_centre'] = 0\n",
    "#     print('Row ', row_index, ', Finished 15/20 amenities')\n",
    "    try:\n",
    "        row['osm_cinema'] = find_amenity_count('cinema', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_cinema'] = 0\n",
    "#     print('Row ', row_index, ', Finished 16/20 amenities')\n",
    "    try:\n",
    "        row['osm_casino'] = find_amenity_count('casino', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_casino'] = 0\n",
    "#     print('Row ', row_index, ', Finished 17/20 amenities')\n",
    "    try:\n",
    "        row['osm_theatre'] = find_amenity_count('theatre', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_theatre'] = 0\n",
    "#     print('Row ', row_index, ', Finished 18/20 amenities')\n",
    "    try:\n",
    "        row['osm_place_of_worship'] = find_amenity_count('place_of_worship', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_place_of_worship'] = 0\n",
    "#     print('Row ', row_index, ', Finished 19/20 amenities')\n",
    "    try:\n",
    "        row['osm_police'] = find_amenity_count('police', row.latitude, row.longitude)\n",
    "    except:\n",
    "        row['osm_police'] = 0\n",
    "#     print('Row ', row_index, ', Finished 20/20 amenities')\n",
    "#     print()\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collection script using the Overpass API has required a total collection time of ~8 hours. Therefore, in order to facilitate swift running of this script, we have executed the script once, after which we have saved the resulting dataset (together will all the augmentation performed previously) into our data folder, and we will load it from there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This data collection function takes ~8 hours to collect all the data from our API into our dataframe\n",
    "# DO NOT EXECUTE AGAIN UNNECESSARILY\n",
    "# augmented_japanese_data = augmented_japanese_data.apply(lambda row: create_amenity_count_columns(row, row.name), axis = 1)\n",
    "\n",
    "# Therefore, we have saved our resulting CSV file, and we will load it here\n",
    "augmented_japanese_data = pd.read_csv('yelp_dataset/augm_japanese_restaurants_osm.csv')\n",
    "augmented_japanese_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have collected is part of the augmentation and feature engineering necessary for having higher chances at detecting patterns within the data with our predictive model later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Augmentation with categorization of `category` column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can see that each Japanese restaurant within our dataset has a set of categories which indicate what kind of cuisine they tackle, and this is a major deciding factor for customers. Therefore, we consider it relevant to include the individual categories within the regression data model. However, since the categories are saved as a list for each row, we will create dummy/categorical columns for each unique possible cuisine category. This is especially helpful since a restaurant often has two or more categories, therefore some data patterns may arise from having them in our model. We expect that the number of customers visiting in the early evening will be influenced by the kind of cuisine and speciality that the restaurant has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per _source: [UCLA Stats](https://stats.idre.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis-2)_: \"categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are.  Instead, they need to be recoded into a series of variables which can then be entered into the regression model.  There are a variety of coding systems that can be used when recoding categorical variables.\"\n",
    "\n",
    "From here on, we will use categorical encoding for representing cuisine categories, attributes of the restaurant, and, later on, for categorizing each weekday in its own column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create categorical variables for each unique category by first finding the set of unique categories, wrangling the names so that they fit proper column naming styles, and then updating their values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_category_set = set()\n",
    "\n",
    "# Here, we iterate over the list of categories from each Japanese business row, \n",
    "# and save the unique set of categories used by our businesses\n",
    "for _, row in augmented_japanese_data.iterrows():\n",
    "    row_category_list = row.categories.split(', ')\n",
    "    unique_category_set = (set(unique_category_set) | set(row_category_list))\n",
    "    \n",
    "unique_category_list = list(unique_category_set)\n",
    "\n",
    "# Finally, we perform string wrangling to write the column names in an appropriate format\n",
    "for i in range(0, len(unique_category_list)):\n",
    "    unique_category_list[i] = unique_category_list[i].lower().replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_').replace('-', '_').replace('&', 'and')\n",
    "\n",
    "print(list(unique_category_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous category set, we need to initialize all new dummy columns in the augmented dataset, which will be having value 0 for absence of the category or 1 for the presence of the category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_categories(row):\n",
    "    row_categories = row.categories.split(', ')\n",
    "    \n",
    "    for category in row_categories:\n",
    "        # We perform the same string wrangling upon the categories to write the column names in an appropriate format\n",
    "        category = category.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_').replace('-', '_').replace('&', 'and')\n",
    "        row[category] = 1\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize new categorical columns inside our augmented dataframe with 0\n",
    "for category in unique_category_list:\n",
    "    augmented_japanese_data[category] = 0\n",
    "\n",
    "# Using the 'mark_categories' function, we mark each row's categories with 1 where it is appropriate\n",
    "augmented_japanese_data = augmented_japanese_data.apply(lambda row: mark_categories(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the cuisine categories have been transformed into dummy variables, let us remove the redundant `category` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data = augmented_japanese_data.drop(columns = 'categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Augmentation with categorization of `attributes` column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `attributes` column holds relevant information about each business in part, such as its facilities, atmosphere and properties. It may be very helpful to encode these string attributes into categorical columns, yet, some of these attributes, in turn, contain more attributes as well. \n",
    "\n",
    "Therefore, we need to be careful and create these columns using Python wrangling upon the attribute values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will want to create categorical columns for each attribute value possible\n",
    "attributes_column_names = set()\n",
    "\n",
    "# Here, we iterate over every row, retrieve its attributes as a dictionary, \n",
    "# and then link the key and value of the attributes together into one column name\n",
    "\n",
    "for entry in augmented_japanese_data.attributes.to_list():\n",
    "    entry = json.loads(entry)\n",
    "    \n",
    "    for key, value in entry.items():\n",
    "        # If the value is a dictionary (a set of more properties),\n",
    "        if (isinstance(value, dict)):\n",
    "            for k, v in value.items():\n",
    "                attributes_column_names.add(str(key + '_' + k + '_' + v))\n",
    "        # Otherwise, the value is a string\n",
    "        else:\n",
    "            attributes_column_names.add(str(key + '_' + value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the new categorical column names, let us join our augmented data with an empty dataframe, after which we initialize all these new categorical column values with 0 as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us join our augmented data with an empty dataframe, \n",
    "# after which we initialize all these new categorical column values with 0 as a starting point\n",
    "\n",
    "augmented_japanese_data = augmented_japanese_data.join(pd.DataFrame(columns = attributes_column_names)).replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to parse each row and find the attributes and values that are present for our business, and using this, we will update the corresponding columns to indicate presence of that category (i.e. updating the column value to 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how the 'attributes' feature is presented for a row\n",
    "client_row.attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_attribute_column_values(row):\n",
    "    # Load the JSON dictionary for the row attributes\n",
    "    entry = json.loads(row.attributes)\n",
    "\n",
    "    for key, value in entry.items():\n",
    "        # If the value is a dictionary (a set of more properties),\n",
    "        if (isinstance(value, dict)):\n",
    "            for k, v in value.items():\n",
    "                row[str(key + '_' + k + '_' + v)] = 1\n",
    "        # Otherwise, if the value is just a string,\n",
    "        else:\n",
    "            row[str(key + '_' + value)] = 1\n",
    "            \n",
    "    # Return the updated row (with attribute category columns updated)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data = augmented_japanese_data.apply(lambda row: update_attribute_column_values(row), axis = 1)\n",
    "augmented_japanese_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Augmentation with categorization of _importance score_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the importance score is actually a categorical interpretation of the different competitor categories, we will need to encode such a variable using one-hot-encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the 'importance' column, and join that to our augmented dataset\n",
    "augmented_japanese_data = augmented_japanese_data.join(pd.get_dummies(augmented_japanese_data.importance, prefix = 'importance_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the importance score has been transformed into categories, let us remove the redundant `importance` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data = augmented_japanese_data.drop(columns = 'importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Augmentation with creating the proportion of customers visiting the business during specific hours (i.e. 16:00-17:00), separated per weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step of our data augmentation, we are interesting in finding out the proportion of customers which came during the hours of 16:00 and 17:00, for each day of the week aggregated across the whole \"checkins\" dataset. We have chosen these specific hours as our reference point, as we are trying to find out whether our client would gain significant increased customer flow and presence if the client were to open its business one hour earlier. Since the data is separated per weekday, and since our client opens every single day at 17:00, we will answer this question by giving a prediction for each weekday independently, in order to see what days of the week does it make sense that our client might open the restaurant earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to compute this proportion for every Japanese restaurant we have in our dataset, we need to perform an __`INNER JOIN`__ between the augmented data and the checkins for all japanese restaurants, joining by the `business_id` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_japanese_checkins_df = pd.read_csv('yelp_dataset/toronto_japanese_checkins.csv')\n",
    "\n",
    "augmented_japanese_data = pd.merge(augmented_japanese_data, \n",
    "                                   toronto_japanese_checkins_df, \n",
    "                                   on = 'business_id', \n",
    "                                   how = 'inner'\n",
    "                                  )\n",
    "augmented_japanese_data = augmented_japanese_data.rename(columns = {'date': 'checkins'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we havev created a function which reads-in every restaurant row of the dataset, checks every one of their checkin times, converts that checkin to a date, and then checks to see if that checkin date is also relevant to the time-slot that we are searching for (16:00-17:00). This function will finally compute the proportion of checkins within the searched timeslot as __the number of checkins within that timeslot for a weekday divided by the total aggregated number of checkins for that weekday.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_customer_proportion_for_hour_and_day(row, hour_string):\n",
    "    desired_hour = datetime.strptime(hour_string, '%H:%M')\n",
    "    \n",
    "    # We will represent the days of the week as the list positions from 1 to 7 (therefore, position 0 is not used)\n",
    "    checkins_desired_hour_list = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    total_checkins_list = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "#     # Create columns corresponding to the days of the week (1 being Monday, up to 7, being Sunday)\n",
    "    weekday_name_list = ['', 'mon', 'tues', 'wed', 'thur', 'fri', 'sat', 'sun']\n",
    "#     for i in range(1, 8):\n",
    "#         row[str(i)] = 0\n",
    "        \n",
    "    checkin_date_list = row.checkins.split(', ')\n",
    "    for date in checkin_date_list:\n",
    "        date = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Let's increment the total number of checkins for the found weekday\n",
    "        # date.isoweekday() returns the day of week in which the checkin was done, from 1 to 7\n",
    "        total_checkins_list[date.isoweekday()] += 1\n",
    "        \n",
    "        # let us increment the checkin counter for the day of the week during which a checkin was done, \n",
    "        # if the checkin was done during the desired hour of the day\n",
    "        if ((date.hour == desired_hour.hour) and (date.minute >= desired_hour.minute)):\n",
    "            checkins_desired_hour_list[date.isoweekday()] += 1\n",
    "            \n",
    "    # Now, we have the total number of checkins per weekday, and the total number of checkins during the desired hour, per weekday\n",
    "    \n",
    "    # We can compute, for each day, what is the proportion of the total checkins that would happen during the desired hour, per weekday\n",
    "    # This result (the proportion) will be saved in the dataframe row, with a column name corresponding to the integer of the corresponding weekday (e.g. column \"1\" for monday)\n",
    "    \n",
    "    for i in range(1, 8):\n",
    "        if (total_checkins_list[i] != 0):\n",
    "            row[weekday_name_list[i]] = (checkins_desired_hour_list[i] / total_checkins_list[i])\n",
    "        else:\n",
    "            row[weekday_name_list[i]] = 0\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data = augmented_japanese_data.apply(lambda row: retrieve_customer_proportion_for_hour_and_day(row, '16:00'), axis = 1)\n",
    "augmented_japanese_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is correct in terms of the calculation, however our model needs to compute one single proportion out of 1 dataset row, and here, we can see that we have all 7 proportions (for all 7 days of the week) in the same row. \n",
    "\n",
    "Therefore, let us apply a pivoting function which ___converts the dataset from a wide format to a long format___ by melting the weekday variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us pivot the augmented data into long format, with one row for each weekday for each restaurant\n",
    "augmented_japanese_data = augmented_japanese_data.melt(id_vars = augmented_japanese_data.columns[0:-7], \n",
    "                                                       value_vars = ['mon', 'tues', 'wed', 'thur', 'fri', 'sat', 'sun'], \n",
    "                                                       var_name = 'weekday', \n",
    "                                                       value_name = 'prop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Now, the total number of observations has multiplied by 7, since each restaurant will have one row in the dataset for each day of the week. \n",
    "\n",
    "However, now the weekdays have been encoded as a separate column, with string values corresponding to the weekday name. Since this is not the right format for a variable to be included in a regression model, we will create \"dummy\" categorical variables representing which of the weekdays each row has data about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create categorical variables for each day of the week (so that they are not considered)\n",
    "augmented_japanese_data = augmented_japanese_data.join(\n",
    "                              pd.get_dummies(augmented_japanese_data['weekday'])[['mon', 'tues', 'wed', 'thur', 'fri', 'sat', 'sun']]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us 'clean up' and remove the old `checkins` and `weekday` variables, which we have used in the process of creating the proportion of customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data = augmented_japanese_data.drop(columns = ['checkins', 'weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_japanese_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g) Non-relevant attribute removal & correlation matrix visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now fully augmented the dataset with as many relevant new data attributes as we could think about, and it is time to create the prediction model. One of the last steps before doing that is to remove some irrelevant attributes that are kept in the dataset, such as `state` or `name`. We could have removed such attributes at any moment in the augmentation process, however we decided not to do so until now, since it was important to check, at every step of the way, that the data was augmented properly for different restaurants, and some of those attributes allowed easy visualization that different data was indeed correctly inputted to different restaurant rows. \n",
    "\n",
    "Let us now first separate our client's augmented data rows from the rest of the business data, and afterwards, drop the unnecessary string-type attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df = augmented_japanese_data[augmented_japanese_data['name'] == client_row['name']]\n",
    "model_df = augmented_japanese_data[augmented_japanese_data['name'] != client_row['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df.drop(columns = ['business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'is_open', 'attributes', 'hours'])\n",
    "\n",
    "# For client_df, we will also drop the proportion, since we are sure that, because the client does not have open between those hours, the proportion will be 0.0 \n",
    "# (that's actually what we're trying to predict)\n",
    "client_df = client_df.drop(columns = ['business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'is_open', 'attributes', 'hours', 'prop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below from the `info()` function of `pandas`, our dataset now has only __`int`__ and __`float`__ attributes, which is exactly how the data should be prepared for a Machine Learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to remember that some of the restaurants in our data do not have open during our specified time (16:00-17:00), for some days of the week. This is probably due to a wide variety of life and business factors, something which is not quantified in the dataset. Therefore, these rows will give a proportion of __0.0__, which will bias the model into thinking that the attribute collinearity may have caused a proportion of 0.0, while this is most likely not true. __This is a good example of a situation where correlation does not imply causation, and in order to not bias the model, we will only keep and consider the data rows for the restaurants and the weekdays where the restaurant actually is open (where the proportion is > 0.0).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df[model_df.prop > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of rows for our modeling dataset is:', model_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing before moving on to Machine Learning: we would like to understand how do the variables in our augmented model dataset correlate with each other. Even though multicollinearity should be well-handled by Neural Networks, it would still be best if our model would have most variables be independent of each other, in hopes that at least some of them will explain the variance in the dependent variable (`prop`).\n",
    "\n",
    "We will use a correlation heatmap using the `seaborn` visualization package. Since there are many attributes to be displayed, please zoom in to see the individual correlation pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [60, 60])\n",
    "sb.heatmap(model_df.corr(), cmap = 'RdYlGn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 main observations that we can discover from the correlation heatmap above:\n",
    "\n",
    "* It seems that our dataset is, for the most part, uncorrelated, with a few subsets of the data having moderate or high correlations due to their nature. The OpenStreetMaps-collected data is one such example of a correlated data subset, since it is reasonable that, if there is more of a type of amenity, there likely are more of multiple types of amenities, since restaurants, bars, cafes and other attractions tend to flock together. Also, some of the encoded business attributes seem to be correlated with each other, and that may be because they are subattribtes coming from the same parent attribute.\n",
    "\n",
    "* There are some attributes for which the correlation matrix indicates vertical/horizontal __white lines__. This represents a __correlation of 0.00 for that variable with any other variable in the dataset, which is only possible if the variable in question is always having the same value for each row of the whole dataset.__ In that case, there is no variance at all for the variable, and, naturally, computing a correlation coefficient against any other variable will give its value as 0.0, since no other variables can influence its singular value. \n",
    "\n",
    "If a variable determines a white line within the correlation matrix, it means that it cannot explain any of the variance of the other variables, therefore it's a good idea to remove these variables from the final modeling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df.drop(columns = ['osm_hospital', 'osm_casino', 'donuts', 'fish_and_chips', 'izakaya', 'chicken_shop', 'restaurants', 'dive_bars', 'education', 'vegetarian', 'seafood_markets', 'kosher', 'hot_pot', 'modern_european', 'japanese', 'BYOB_True', 'Ambience_touristy_True', 'importance_score_6'])\n",
    "client_df = client_df.drop(columns = ['osm_hospital', 'osm_casino', 'donuts', 'fish_and_chips', 'izakaya', 'chicken_shop', 'restaurants', 'dive_bars', 'education', 'vegetarian', 'seafood_markets', 'kosher', 'hot_pot', 'modern_european', 'japanese', 'BYOB_True', 'Ambience_touristy_True', 'importance_score_6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation for Model Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step for achieving regression predictions upon the variable _`prop`_ is to transform our dataset from a Pandas dataframe into matrixes of attributes `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_df.drop(columns = 'prop').to_numpy()\n",
    "y = model_df['prop'].to_numpy().reshape((-1, 1))\n",
    "\n",
    "X_client = client_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the only step needed to be done to have the model ready for ingestion. We will _not_ scale our data here, since we will make use of `sklearn`'s custom Regressors which automatically scale and inverse-scale the data at fitting, respectively at prediction times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prediction Model: _Linear Regression_ baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some of our attributes have a widely different scale (e.g. `review_count`) than others (e.g. any categorical encoded variable), we draw the attention upon the problem of __feature scaling__. Scaling may be important for the correct updating of the weights in the stochastic gradient descent algorithm for Neural Networks, as well as for the coefficients/weights of the linear regression model.\n",
    "\n",
    "For our dataset, we will use a Robust Scaler to scaler our data accordingly, since RobustScaler uses the IQR (interquartile range) of a variable, therefore working best in situations where we have skewed distributions which may contain outliers.\n",
    "\n",
    "_It is extremely important to note the correct way of doing feature scaling: our augmented dataset is our training data, and our client data is our final testing data. Since we consider to only have the training data available to us for usage and modeling, __we will fit a feature scaler on the training data, and use the training scaler upon the testing set to scale the data__. This is very important, as we cannot include our test data in the training scaler, nor should we have a separate scaler for testing and another for training data, since they probably don't transform the data with exactly the same parameters._\n",
    "\n",
    "For this transformation, we are using a `TransformedTargetRegressor()`, which takes in the regressor that we are fitting (_i.e. our linear regression_) and a transformer (_i.e. the RobustScaler_) which scales the data before fitting, and inverse-transforms it after predicting. For our case scenario of proportion regression, returning back the linear model output in the original scale is essential, in order to easily interpret the predicted proportion data from the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the two error measures that we will use for computing the efficiency of the model. Since we are working with measurable __proportions, which take values between 0.00 and 1.00__, MAE (Mean Absolute Error) is a better loss function to interpret than the RMSE. However, we will keep both for consistency reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_pred, y_true):\n",
    "    return np.sqrt(np.mean((y_pred - y_true)**2))\n",
    "\n",
    "def MAE(y_pred, y_true):\n",
    "    return np.mean(np.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following setup uses __K-fold cross-validation__, with 20 folds, in order to compute a mean of RMSE and MAE errors for __two linear models__: \n",
    "\n",
    "* one which performs regression on the scaled data, and \n",
    "* one which computes on non-scaled data, \n",
    "\n",
    "in order to see whether there is any real improvement for our specific dataset whether we scale the data or not.\n",
    "\n",
    "The `random_state` variable is set within the CV fold splitting object, in order to create reproducibility in the context of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-validation fold object which will give CV folds\n",
    "KFold_CV = KFold(n_splits = 20, shuffle = True, random_state = 42)\n",
    "\n",
    "# Create vectors to store the computed errors for each CV fold, both training and test errors\n",
    "scaled_RMSE_train_values = []\n",
    "scaled_MAE_train_values = []\n",
    "scaled_RMSE_test_values = []\n",
    "scaled_MAE_test_values = []\n",
    "\n",
    "non_scaled_RMSE_train_values = []\n",
    "non_scaled_MAE_train_values = []\n",
    "non_scaled_RMSE_test_values = []\n",
    "non_scaled_MAE_test_values = []\n",
    "\n",
    "# From our full data matrix X, we create training and test data using cross-validation\n",
    "for train_index, test_index in KFold_CV.split(X):\n",
    "    \n",
    "    # Create the training set from the CV training indexes\n",
    "    X_train, y_train = X[train_index, :], y[train_index]\n",
    "    \n",
    "    # Create the test set from the CV test indexes\n",
    "    X_test, y_test = X[test_index, :], y[test_index]\n",
    "    \n",
    "    # This is our linear model which works automatically upon SCALED data\n",
    "    tt_lr_model = TransformedTargetRegressor(regressor = LinearRegression(fit_intercept = True, n_jobs = -1), \n",
    "                                             transformer = RobustScaler())\n",
    "    \n",
    "    # This is the linear regression model which fits on NON-SCALED data\n",
    "    lr_model = LinearRegression(fit_intercept = True, n_jobs = -1)\n",
    "    \n",
    "    # We train both our models on the non-scaled data, and the TransformedTargetRegressor will automatically use its transformer to scale the data\n",
    "    tt_lr_model.fit(X_train, y_train)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    non_scaled_RMSE_train_values.append(RMSE(lr_model.predict(X_train), y_train))\n",
    "    non_scaled_MAE_train_values.append(MAE(lr_model.predict(X_train), y_train))\n",
    "    #---------------------------------------------------------------------------#\n",
    "    non_scaled_RMSE_test_values.append(RMSE(lr_model.predict(X_test), y_test))\n",
    "    non_scaled_MAE_test_values.append(MAE(lr_model.predict(X_test), y_test))\n",
    "    \n",
    "    scaled_RMSE_train_values.append(RMSE(tt_lr_model.predict(X_train), y_train))\n",
    "    scaled_MAE_train_values.append(MAE(tt_lr_model.predict(X_train), y_train))\n",
    "    #--------------------------------------------------------------------------#\n",
    "    scaled_RMSE_test_values.append(RMSE(tt_lr_model.predict(X_test), y_test))\n",
    "    scaled_MAE_test_values.append(MAE(tt_lr_model.predict(X_test), y_test))\n",
    "\n",
    "    \n",
    "print('TRAIN SET RMSE:')\n",
    "print('(Mean: Train RMSE - NON-SCALED data) for linear regression upon the japanese business dataset: ', str(np.mean(non_scaled_RMSE_train_values)))\n",
    "print('(Std:  Train RMSE - NON-SCALED data) for linear regression upon the japanese business dataset: ', str(np.std(non_scaled_RMSE_train_values)))\n",
    "print('(Mean: Train RMSE - SCALED data) for linear regression upon the japanese business dataset:     ', str(np.mean(scaled_RMSE_train_values)))\n",
    "print('(Std:  Train RMSE - SCALED data) for linear regression upon the japanese business dataset:     ', str(np.std(scaled_RMSE_train_values)))\n",
    "print()\n",
    "print('TEST SET RMSE:')\n",
    "print('(Mean: Test RMSE - NON-SCALED data) for  linear regression upon the japanese business dataset: ', str(np.mean(non_scaled_RMSE_test_values)))\n",
    "print('(Std:  Test RMSE - NON-SCALED data) for  linear regression upon the japanese business dataset: ', str(np.std(non_scaled_RMSE_test_values)))\n",
    "print('(Mean: Test RMSE - SCALED data) for  linear regression upon the japanese business dataset:     ', str(np.mean(scaled_RMSE_test_values)))\n",
    "print('(Std:  Test RMSE - SCALED data) for  linear regression upon the japanese business dataset:     ', str(np.std(scaled_RMSE_test_values)))\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print('TRAIN SET MAE:')\n",
    "print('(Mean: Train MAE - NON-SCALED data) for linear regression upon the japanese business dataset:  ', str(np.mean(non_scaled_MAE_train_values)))\n",
    "print('(Std:  Train MAE - NON-SCALED data) for linear regression upon the japanese business dataset:  ', str(np.std(non_scaled_MAE_train_values)))\n",
    "print('(Mean: Train MAE - SCALED data) for linear regression upon the japanese business dataset:      ', str(np.mean(scaled_MAE_train_values)))\n",
    "print('(Std:  Train MAE - SCALED data) for linear regression upon the japanese business dataset:      ', str(np.std(scaled_MAE_train_values)))\n",
    "print()\n",
    "print('TEST SET MAE:')\n",
    "print('(Mean: Test MAE - NON-SCALED data) for linear regression upon the japanese business dataset:   ', str(np.mean(non_scaled_MAE_test_values)))\n",
    "print('(Std:  Test MAE - NON-SCALED data) for linear regression upon the japanese business dataset:   ', str(np.std(non_scaled_MAE_test_values)))\n",
    "print('(Mean: Test MAE - SCALED data) for linear regression upon the japanese business dataset:       ', str(np.mean(scaled_MAE_test_values)))\n",
    "print('(Std:  Test MAE - SCALED data) for linear regression upon the japanese business dataset:       ', str(np.std(scaled_MAE_test_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the linear model prediction seem to be reliable, because, __in both scaled & non-scaled scenarios, across 20 CV folds, the training error and testing error are very close to each other. This indicates that the model is not doing large amounts of overfitting__.\n",
    "\n",
    "Moreover, if we look at the testing set difference in errors between scaled and non-scaled data, both RMSE and MAE coincide perfectly until the 15th decimal! <br> This shows that, using a linear model for our data, the model does not seem to take into account the features with large scale differences compared to our categorical variables (having values between 0 and 1).\n",
    "\n",
    "__Lastly, it is important to understand that our Test set MAE average error of $\\mu = $ 0.09, with a standard deviation $\\sigma = $ 0.005 means that, when predicting for our client the average proportion of customers that would come between 16:00 and 17:00, such predictions will have a margin of error between ($\\mu-3\\sigma, \\mu+3\\sigma$). More specifically, the prediction that our client would receive from the linear model would be between $7.5\\% - 10.5\\%$ lower or higher.__ This error marging would surely decrease, given more data to be fed into the model, since reaching a low MAE from only 4000 entries is an expected challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prediction Model: _Neural Networks_ model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Final client proportion prediction & Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
