{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Business Analytics 2020 - _Notebook 1/3: <u>Yelp dataset segmentation & wrangling</u>_\n",
    "## _Subsetting the Yelp dataset to include only data from Toronto, Canada, and wrangling the business data_\n",
    "---\n",
    "## Table of Contents:\n",
    "- [__Part 1: Subsetting the Yelp data__](#1.)\n",
    "- [__Part 2: Wrangling the Yelp business data__](#2.)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import pandas as pd \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The purpose of this notebook has been to create a standarized method to perform data segmentation across all future analysis parts of our notebook, so that all analysis be done, as a starting point, upon the same data subsets for all project parts.__\n",
    "\n",
    "When there has been an individual need for further data wrangling or segmentation, this is done and discussed subsequently inside the relevant project subpart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.'></a>\n",
    "## Part 1: Subsetting the Yelp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Here, we are filtering the 10GB+ original dataset into the relevant subset of businesses and saving the corresponding data subsets as a CSV file for easier loading of the dataset:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Filtering businesses:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the businesses dataframe using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_business_df = pd.read_json('yelp_dataset/yelp_academic_dataset_business.json', lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we are first interested in focusing our analysis on a specific city chosen for this project: **Toronto**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_business_toronto_df = all_business_df[all_business_df.city == 'Toronto']\n",
    "all_business_toronto_df = all_business_toronto_df.loc[all_business_toronto_df.categories.dropna().index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to save a dataset relative to all the restaurants in Toronto, in order to possibly use it for extended competitor analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_business_toronto_df = all_business_toronto_df[all_business_toronto_df.categories.str.contains('Restaurant')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are interested in selecting all businesses which deal with Japanese food:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_restaurants_business_toronto_df = restaurants_business_toronto_df[restaurants_business_toronto_df.categories.str.contains('Japanese')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, now that we have our relevant subset of business that we can potentially look at, we will try to create a list of the IDs of these businesses, in order to use this to subset the other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_id_all_business_toronto = list(all_business_toronto_df.business_id)\n",
    "business_id_restaurants_toronto = list(restaurants_business_toronto_df.business_id)\n",
    "business_id_japanese_restaurants_toronto = list(japanese_restaurants_business_toronto_df.business_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all these datasets have been subsetted, let us save them into their own CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_business_toronto_df.to_csv('yelp_dataset/toronto_all_business.csv', index = False)\n",
    "restaurants_business_toronto_df.to_csv('yelp_dataset/toronto_restaurant_business.csv', index = False)\n",
    "japanese_restaurants_business_toronto_df.to_csv('yelp_dataset/toronto_japanese_business.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_business_df\n",
    "del all_business_toronto_df\n",
    "del restaurants_business_toronto_df\n",
    "del japanese_restaurants_business_toronto_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Filtering reviews:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *review* JSON file is extremely large, and often is bound to give memory errors when trying to read it directly from using the specialized `read_json` functions of `pandas` or `dask`. Therefore, we need a more Python-traditional approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yelp_dataset/yelp_academic_dataset_review.json', mode = 'r', encoding = 'utf8') as json_file:\n",
    "    data = json_file.readlines()\n",
    "    data = list(map(json.loads, data))\n",
    "    \n",
    "all_reviews_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we are able to load the data, we will filter the reviews immediately by keeping only those that are relevant to our businesses, in order to lower the memory footprint that is taken up by the *reviews* dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we will filter by all reviews relevant to all businesses in **Toronto**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_toronto_df = all_reviews_df[all_reviews_df.business_id.isin(business_id_all_business_toronto)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to subset by the reviews of all restaurants in Toronto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_reviews_toronto_df = all_reviews_toronto_df[all_reviews_toronto_df.business_id.isin(business_id_restaurants_toronto)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to subset even further to only get the reviews of any Japanese-specific restaurants in Toronto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_restaurants_reviews_toronto_df = restaurants_reviews_toronto_df[restaurants_reviews_toronto_df.business_id.isin(business_id_japanese_restaurants_toronto)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we are interested in storing the IDs of the users writing all these reviews, in order to filter the Tips and Users datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_all_reviews_toronto = list(all_reviews_toronto_df.user_id)\n",
    "user_id_restaurants_toronto = list(restaurants_reviews_toronto_df.user_id)\n",
    "user_id_japanese_restaurants_toronto = list(japanese_restaurants_reviews_toronto_df.user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save these three datasets for future re-usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_toronto_df.to_csv('yelp_dataset/toronto_all_reviews.csv', index = False)\n",
    "restaurants_reviews_toronto_df.to_csv('yelp_dataset/toronto_restaurant_reviews.csv', index = False)\n",
    "japanese_restaurants_reviews_toronto_df.to_csv('yelp_dataset/toronto_japanese_reviews.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_reviews_df\n",
    "del all_reviews_toronto_df\n",
    "del restaurants_reviews_toronto_df\n",
    "del japanese_restaurants_reviews_toronto_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Filtering checkins:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same steps will be repeated for the check-in dataset, so let's follow along:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkins_df = pd.read_json('yelp_dataset/yelp_academic_dataset_checkin.json', lines = True)\n",
    "all_checkins_toronto_df = all_checkins_df[all_checkins_df.business_id.isin(business_id_all_business_toronto)]\n",
    "restaurants_checkins_toronto_df = all_checkins_toronto_df[all_checkins_toronto_df.business_id.isin(business_id_restaurants_toronto)]\n",
    "japanese_restaurants_checkins_toronto_df = restaurants_checkins_toronto_df[restaurants_checkins_toronto_df.business_id.isin(business_id_japanese_restaurants_toronto)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkins_toronto_df.to_csv('yelp_dataset/toronto_all_checkins.csv', index = False)\n",
    "restaurants_checkins_toronto_df.to_csv('yelp_dataset/toronto_restaurant_checkins.csv', index = False)\n",
    "japanese_restaurants_checkins_toronto_df.to_csv('yelp_dataset/toronto_japanese_checkins.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_checkins_df\n",
    "del all_checkins_toronto_df\n",
    "del restaurants_checkins_toronto_df\n",
    "del japanese_restaurants_checkins_toronto_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Filtering tips:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same steps as last time will be used for the tips dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tips_df = pd.read_json('yelp_dataset/yelp_academic_dataset_tip.json', lines = True)\n",
    "all_tips_toronto_df = all_tips_df[(all_tips_df.business_id.isin(business_id_all_business_toronto) & (all_tips_df.user_id.isin(user_id_all_reviews_toronto)))]\n",
    "restaurants_tips_toronto_df = all_tips_toronto_df[(all_tips_toronto_df.business_id.isin(business_id_restaurants_toronto) & all_tips_toronto_df.user_id.isin(user_id_restaurants_toronto))]\n",
    "japanese_restaurants_tips_toronto_df = restaurants_tips_toronto_df[(restaurants_tips_toronto_df.business_id.isin(business_id_japanese_restaurants_toronto) & restaurants_tips_toronto_df.user_id.isin(user_id_japanese_restaurants_toronto))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tips_toronto_df.to_csv('yelp_dataset/toronto_all_tips.csv', index = False)\n",
    "restaurants_tips_toronto_df.to_csv('yelp_dataset/toronto_restaurant_tips.csv', index = False)\n",
    "japanese_restaurants_tips_toronto_df.to_csv('yelp_dataset/toronto_japanese_tips.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_tips_df\n",
    "del all_tips_toronto_df\n",
    "del restaurants_tips_toronto_df\n",
    "del japanese_restaurants_tips_toronto_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Filtering users:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once again, the same steps will be applied for the users dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yelp_dataset/yelp_academic_dataset_user.json', mode = 'r', encoding = 'utf8') as json_file:\n",
    "    data = json_file.readlines()\n",
    "    data = list(map(json.loads, data))\n",
    "    \n",
    "all_users_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users_toronto_df = all_users_df[all_users_df.user_id.isin(user_id_all_reviews_toronto)]\n",
    "restaurants_users_toronto_df = all_users_toronto_df[all_users_toronto_df.user_id.isin(user_id_restaurants_toronto)]\n",
    "japanese_restaurants_users_toronto_df = restaurants_users_toronto_df[restaurants_users_toronto_df.user_id.isin(user_id_japanese_restaurants_toronto)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users_toronto_df.to_csv('yelp_dataset/toronto_all_users.csv', index = False)\n",
    "restaurants_users_toronto_df.to_csv('yelp_dataset/toronto_restaurant_users.csv', index = False)\n",
    "japanese_restaurants_users_toronto_df.to_csv('yelp_dataset/toronto_japanese_users.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_users_df\n",
    "del all_users_toronto_df\n",
    "del restaurants_users_toronto_df\n",
    "del japanese_restaurants_users_toronto_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.'></a>\n",
    "## Part 2: Wrangling the Yelp business data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_all_business_df = pd.read_csv('yelp_dataset/toronto_all_business.csv')\n",
    "toronto_restaurant_business_df = pd.read_csv('yelp_dataset/toronto_restaurant_business.csv')\n",
    "toronto_japanese_business_df = pd.read_csv('yelp_dataset/toronto_japanese_business.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangle the `attributes` column into a JSON-parsable format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_attributes(row):\n",
    "    attributes_data = row.attributes\n",
    "    \n",
    "    if (pd.isnull(attributes_data)):\n",
    "        row['attributes'] = '{}'\n",
    "        return row\n",
    "        \n",
    "    attributes_data = attributes_data.replace(\"\\\"u\\'\", \"\\'\")\n",
    "    attributes_data = attributes_data.replace('\\'', '\\\"')\n",
    "    attributes_data = attributes_data.replace('\"\"', '\"')\n",
    "    attributes_data = attributes_data.replace('\"{', '{')\n",
    "    attributes_data = attributes_data.replace('}\"', '}')\n",
    "    attributes_data = attributes_data.replace(' False', ' \"False\"')\n",
    "    attributes_data = attributes_data.replace(' True', ' \"True\"')\n",
    "    attributes_data = attributes_data.replace(' None', ' \"None\"')\n",
    "    \n",
    "    row['attributes'] = attributes_data\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a test to see if all rows in the largest dataset can be easily converted to JSON format.\n",
    "\n",
    "This code should finish without showing errors, if all the data can be safely converted into JSON dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a test to see if all rows in the largest dataset can be easily converted to JSON format\n",
    "# This code should finish without showing errors, if all the data can be safely converted into JSON dictionaries\n",
    "\n",
    "for index, row in toronto_all_business_df.apply(lambda row: wrangle_attributes(row), axis = 1).iterrows():\n",
    "    json.loads(row.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_all_business_df = toronto_all_business_df.apply(lambda row: wrangle_attributes(row), axis = 1)\n",
    "toronto_restaurant_business_df = toronto_restaurant_business_df.apply(lambda row: wrangle_attributes(row), axis = 1)\n",
    "toronto_japanese_business_df = toronto_japanese_business_df.apply(lambda row: wrangle_attributes(row), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangle the `hours` column into a fixed and consistent hour format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_hours(row):\n",
    "    hours_data = row.hours\n",
    "    \n",
    "    if (pd.isnull(hours_data)):\n",
    "        row['hours'] = '{}'\n",
    "        return row\n",
    "        \n",
    "    hours_data = hours_data.replace('\\'', '\\\"')\n",
    "    hours_data = hours_data.replace('\"\"', '\"')\n",
    "    hours_data = hours_data.replace('\"{', '{')\n",
    "    hours_data = hours_data.replace('}\"', '}')\n",
    "    hours_data = hours_data.replace(':0', ':00')\n",
    "    \n",
    "    row['hours'] = hours_data\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a test to see if all rows in the largest dataset can be easily converted to JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a test to see if all rows in the largest dataset can be easily converted to JSON format\n",
    "\n",
    "for index, row in toronto_all_business_df.apply(lambda row: wrangle_hours(row), axis = 1).iterrows():\n",
    "    json.loads(row.hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_all_business_df = toronto_all_business_df.apply(lambda row: wrangle_hours(row), axis = 1)\n",
    "toronto_restaurant_business_df = toronto_restaurant_business_df.apply(lambda row: wrangle_hours(row), axis = 1)\n",
    "toronto_japanese_business_df = toronto_japanese_business_df.apply(lambda row: wrangle_hours(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_all_business_df.to_csv('yelp_dataset/toronto_all_business.csv', index = False)\n",
    "toronto_restaurant_business_df.to_csv('yelp_dataset/toronto_restaurant_business.csv', index = False)\n",
    "toronto_japanese_business_df.to_csv('yelp_dataset/toronto_japanese_business.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
